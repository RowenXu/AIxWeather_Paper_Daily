<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 19 Feb 2026 04:22:00 +0000</lastBuildDate>
    <item>
      <title>Creating a digital poet</title>
      <link>http://arxiv.org/abs/2602.16578v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vered Tohar, Tsahi Hayat, Amir Leshem&lt;/p&gt;&lt;p&gt;Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16578v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:25:10 +0000</pubDate>
    </item>
    <item>
      <title>AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS</title>
      <link>http://arxiv.org/abs/2602.16579v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Maria Luisa Taccari, Kenza Tazi, Oisín M. Morrison, Andreas Grafberger, Juan Colonese, Corentin Carton de Wiart, Christel Prudhomme, Cinzia Mazzetti&lt;/p&gt;&lt;p&gt;Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16579v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:26:36 +0000</pubDate>
    </item>
    <item>
      <title>DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows</title>
      <link>http://arxiv.org/abs/2602.16585v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dimitri Yatsenko, Thinh T. Nguyen&lt;/p&gt;&lt;p&gt;Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16585v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:35:47 +0000</pubDate>
    </item>
    <item>
      <title>A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification</title>
      <link>http://arxiv.org/abs/2602.16590v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qi You, Yitai Cheng, Zichao Zeng, James Haworth&lt;/p&gt;&lt;p&gt;Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in com&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16590v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:41:32 +0000</pubDate>
    </item>
    <item>
      <title>Sequential Membership Inference Attacks</title>
      <link>http://arxiv.org/abs/2602.16596v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Thomas Michel, Debabrota Basu, Emilie Kaufmann&lt;/p&gt;&lt;p&gt;Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16596v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:51:13 +0000</pubDate>
    </item>
    <item>
      <title>Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study</title>
      <link>http://arxiv.org/abs/2602.16601v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Nail B. Khelifa, Richard E. Turner, Ramji Venkataramanan&lt;/p&gt;&lt;p&gt;Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16601v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:56:36 +0000</pubDate>
    </item>
    <item>
      <title>FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving</title>
      <link>http://arxiv.org/abs/2602.16603v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, Lijie Wen&lt;/p&gt;&lt;p&gt;The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency,&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16603v1</guid>
      <pubDate>Wed, 18 Feb 2026 16:57:45 +0000</pubDate>
    </item>
    <item>
      <title>Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models</title>
      <link>http://arxiv.org/abs/2602.16608v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Melkamu Abay Mersha, Jugal Kalita&lt;/p&gt;&lt;p&gt;Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16608v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:03:10 +0000</pubDate>
    </item>
    <item>
      <title>Who can we trust? LLM-as-a-jury for Comparative Assessment</title>
      <link>http://arxiv.org/abs/2602.16610v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, Kate M. Knill&lt;/p&gt;&lt;p&gt;Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16610v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:04:02 +0000</pubDate>
    </item>
    <item>
      <title>Causal and Compositional Abstraction</title>
      <link>http://arxiv.org/abs/2602.16612v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Robin Lorenz, Sean Tull&lt;/p&gt;&lt;p&gt;Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, a&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16612v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:06:09 +0000</pubDate>
    </item>
    <item>
      <title>A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models</title>
      <link>http://arxiv.org/abs/2602.16626v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; SungJun Cho, Chetan Gohil, Rukuang Huang, Oiwi Parker Jones, Mark W. Woolrich&lt;/p&gt;&lt;p&gt;Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16626v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:21:02 +0000</pubDate>
    </item>
    <item>
      <title>Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models</title>
      <link>http://arxiv.org/abs/2602.16634v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yu Xie, Ludwig Winkler, Lixin Sun, Sarah Lewis, Adam E. Foster, José Jiménez Luna, Tim Hempel, Michael Gastegger&lt;/p&gt;&lt;p&gt;The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16634v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:26:15 +0000</pubDate>
    </item>
    <item>
      <title>Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System</title>
      <link>http://arxiv.org/abs/2602.16650v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sonakshi Gupta, Akhlak Mahmood, Wei Xiong, Rampi Ramprasad&lt;/p&gt;&lt;p&gt;Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is r&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16650v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:46:09 +0000</pubDate>
    </item>
    <item>
      <title>Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments</title>
      <link>http://arxiv.org/abs/2602.16653v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yangjie Xu, Lujun Li, Lama Sleem, Niccolo Gentile, Yewei Song, Yiqun Wang, Siming Ji, Wenbo Wu&lt;/p&gt;&lt;p&gt;Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs oft&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16653v1</guid>
      <pubDate>Wed, 18 Feb 2026 17:52:17 +0000</pubDate>
    </item>
    <item>
      <title>Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment</title>
      <link>http://arxiv.org/abs/2602.16660v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai&lt;/p&gt;&lt;p&gt;The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16660v1</guid>
      <pubDate>Wed, 18 Feb 2026 18:01:23 +0000</pubDate>
    </item>
    <item>
      <title>Towards a Science of AI Agent Reliability</title>
      <link>http://arxiv.org/abs/2602.16666v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan&lt;/p&gt;&lt;p&gt;AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16666v1</guid>
      <pubDate>Wed, 18 Feb 2026 18:05:44 +0000</pubDate>
    </item>
    <item>
      <title>SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation</title>
      <link>http://arxiv.org/abs/2602.16671v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand&lt;/p&gt;&lt;p&gt;Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelev&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16671v1</guid>
      <pubDate>Wed, 18 Feb 2026 18:09:03 +0000</pubDate>
    </item>
    <item>
      <title>Synthetic-Powered Multiple Testing with FDR Control</title>
      <link>http://arxiv.org/abs/2602.16690v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yonghoon Lee, Meshi Bashari, Edgar Dobriban, Yaniv Romano&lt;/p&gt;&lt;p&gt;Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16690v1</guid>
      <pubDate>Wed, 18 Feb 2026 18:36:24 +0000</pubDate>
    </item>
    <item>
      <title>Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology</title>
      <link>http://arxiv.org/abs/2602.16703v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shen Zhou Hong, Alex Kleinman, Alyssa Mathiowetz, Adam Howes, Julian Cohen, Suveer Ganta, Alex Letizia, Dora Liao&lt;/p&gt;&lt;p&gt;Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16703v1</guid>
      <pubDate>Wed, 18 Feb 2026 18:51:28 +0000</pubDate>
    </item>
    <item>
      <title>Policy Compiler for Secure Agentic Systems</title>
      <link>http://arxiv.org/abs/2602.16708v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Nils Palumbo, Sarthak Choudhary, Jihye Choi, Prasad Chalasani, Mihai Christodorescu, Somesh Jha&lt;/p&gt;&lt;p&gt;LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.16708v1</guid>
      <pubDate>Wed, 18 Feb 2026 18:57:12 +0000</pubDate>
    </item>
  </channel>
</rss>
