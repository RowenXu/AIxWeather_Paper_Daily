<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 10 Dec 2025 03:23:56 +0000</lastBuildDate>
    <item>
      <title>Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?</title>
      <link>http://arxiv.org/abs/2512.08798v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jeongwhan Choi, Woosung Kang, Minseo Kim, Jongwoo Kim, Noseong Park&lt;/p&gt;&lt;p&gt;Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08798v1</guid>
      <pubDate>Tue, 09 Dec 2025 16:51:30 +0000</pubDate>
    </item>
    <item>
      <title>Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework</title>
      <link>http://arxiv.org/abs/2512.08802v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sadegh Momeni, Ge Zhang, Birkett Huber, Hamza Harkous, Sam Lipton, Benoit Seguin, Yanis Pavlidis&lt;/p&gt;&lt;p&gt;Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08802v1</guid>
      <pubDate>Tue, 09 Dec 2025 16:58:08 +0000</pubDate>
    </item>
    <item>
      <title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title>
      <link>http://arxiv.org/abs/2512.08809v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yi Liu, Weixiang Han, Chengjun Cai, Xingliang Yuan, Cong Wang&lt;/p&gt;&lt;p&gt;With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08809v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:03:59 +0000</pubDate>
    </item>
    <item>
      <title>Multicalibration for LLM-based Code Generation</title>
      <link>http://arxiv.org/abs/2512.08810v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Viola Campos, Robin Kuschnereit, Adrian Ulges&lt;/p&gt;&lt;p&gt;As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08810v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:04:01 +0000</pubDate>
    </item>
    <item>
      <title>Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis</title>
      <link>http://arxiv.org/abs/2512.08819v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ferdinand Kapl, Emmanouil Angelis, Tobias Höppe, Kaitlin Maile, Johannes von Oswald, Nino Scherrer, Stefan Bauer&lt;/p&gt;&lt;p&gt;Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08819v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:12:04 +0000</pubDate>
    </item>
    <item>
      <title>Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2512.08820v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yi Zhang, Chun-Wun Cheng, Junyi He, Ke Yu, Yushun Tang, Carola-Bibiane Schönlieb, Zhihai He, Angelica I. Aviles-Rivero&lt;/p&gt;&lt;p&gt;Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08820v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:12:22 +0000</pubDate>
    </item>
    <item>
      <title>CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale</title>
      <link>http://arxiv.org/abs/2512.08826v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shahar Sarfaty, Adi Haviv, Uri Hacohen, Niva Elkin-Koren, Roi Livni, Amit H. Bermano&lt;/p&gt;&lt;p&gt;The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08826v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:15:32 +0000</pubDate>
    </item>
    <item>
      <title>Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference</title>
      <link>http://arxiv.org/abs/2512.08828v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Swaraj Bose, Walter Dempsey&lt;/p&gt;&lt;p&gt;Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08828v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:18:09 +0000</pubDate>
    </item>
    <item>
      <title>InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</title>
      <link>http://arxiv.org/abs/2512.08829v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hongyuan Tao, Bencheng Liao, Shaoyu Chen, Haoran Yin, Qian Zhang, Wenyu Liu, Xinggang Wang&lt;/p&gt;&lt;p&gt;Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08829v1</guid>
      <pubDate>Tue, 09 Dec 2025 17:18:32 +0000</pubDate>
    </item>
    <item>
      <title>Differentially Private Synthetic Data Generation Using Context-Aware GANs</title>
      <link>http://arxiv.org/abs/2512.08869v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Anantaa Kotal, Anupam Joshi&lt;/p&gt;&lt;p&gt;The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08869v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:02:34 +0000</pubDate>
    </item>
    <item>
      <title>Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning</title>
      <link>http://arxiv.org/abs/2512.08873v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jing Jie Tan, Anissa Mokraoui, Ban-Hoe Kwan, Danny Wee-Kiat Ng, Yan-Chai Hum&lt;/p&gt;&lt;p&gt;Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08873v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:05:59 +0000</pubDate>
    </item>
    <item>
      <title>When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</title>
      <link>http://arxiv.org/abs/2512.08875v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Joshua Ward, Bochao Gu, Chi-Hua Wang, Guang Cheng&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08875v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:06:31 +0000</pubDate>
    </item>
    <item>
      <title>DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process</title>
      <link>http://arxiv.org/abs/2512.08879v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mohammad Abu-Shaira, Ajita Rattani, Weishi Shi&lt;/p&gt;&lt;p&gt;Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08879v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:12:38 +0000</pubDate>
    </item>
    <item>
      <title>No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers</title>
      <link>http://arxiv.org/abs/2512.08889v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Damiano Marsili, Georgia Gkioxari&lt;/p&gt;&lt;p&gt;Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08889v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:30:23 +0000</pubDate>
    </item>
    <item>
      <title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title>
      <link>http://arxiv.org/abs/2512.08892v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Guangzhi Xiong, Zhenghao He, Bohan Liu, Sanchit Sinha, Aidong Zhang&lt;/p&gt;&lt;p&gt;Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08892v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:33:22 +0000</pubDate>
    </item>
    <item>
      <title>Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training</title>
      <link>http://arxiv.org/abs/2512.08894v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jakub Krajewski, Amitis Shidani, Dan Busbridge, Sam Wiseman, Jason Ramapuram&lt;/p&gt;&lt;p&gt;While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08894v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:33:48 +0000</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning of Density Estimates with Topological Optimization</title>
      <link>http://arxiv.org/abs/2512.08895v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Suina Tanweer, Firas A. Khasawneh&lt;/p&gt;&lt;p&gt;Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08895v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:35:51 +0000</pubDate>
    </item>
    <item>
      <title>SAQ: Stabilizer-Aware Quantum Error Correction Decoder</title>
      <link>http://arxiv.org/abs/2512.08914v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; David Zenati, Eliya Nachmani&lt;/p&gt;&lt;p&gt;Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08914v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:51:35 +0000</pubDate>
    </item>
    <item>
      <title>Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs</title>
      <link>http://arxiv.org/abs/2512.08923v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Angela van Sprang, Laurens Samson, Ana Lucic, Erman Acar, Sennay Ghebreab, Yuki M. Asano&lt;/p&gt;&lt;p&gt;We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08923v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:57:07 +0000</pubDate>
    </item>
    <item>
      <title>Astra: General Interactive World Model with Autoregressive Denoising</title>
      <link>http://arxiv.org/abs/2512.08931v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yixuan Zhu, Jiaqi Feng, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu&lt;/p&gt;&lt;p&gt;Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.08931v1</guid>
      <pubDate>Tue, 09 Dec 2025 18:59:57 +0000</pubDate>
    </item>
  </channel>
</rss>
