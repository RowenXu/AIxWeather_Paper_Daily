<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 28 Feb 2026 03:56:17 +0000</lastBuildDate>
    <item>
      <title>Mitigating Legibility Tax with Decoupled Prover-Verifier Games</title>
      <link>http://arxiv.org/abs/2602.23248v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yegon Kim, Juho Lee&lt;/p&gt;&lt;p&gt;As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23248v1</guid>
      <pubDate>Thu, 26 Feb 2026 17:25:22 +0000</pubDate>
    </item>
    <item>
      <title>Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2602.23259v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jiangxin Sun, Feng Xue, Teng Long, Chang Liu, Jian-Fang Hu, Wei-Shi Zheng, Nicu Sebe&lt;/p&gt;&lt;p&gt;With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of "only driving like the expert" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the abs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23259v1</guid>
      <pubDate>Thu, 26 Feb 2026 17:32:30 +0000</pubDate>
    </item>
    <item>
      <title>Evaluating Stochasticity in Deep Research Agents</title>
      <link>http://arxiv.org/abs/2602.23271v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haotian Zhai, Elias Stengel-Eskin, Pratik Patil, Liu Leqi&lt;/p&gt;&lt;p&gt;Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23271v1</guid>
      <pubDate>Thu, 26 Feb 2026 17:46:42 +0000</pubDate>
    </item>
    <item>
      <title>CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays</title>
      <link>http://arxiv.org/abs/2602.23276v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hyungyung Lee, Hangyul Yoon, Edward Choi&lt;/p&gt;&lt;p&gt;Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinic&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23276v1</guid>
      <pubDate>Thu, 26 Feb 2026 17:51:21 +0000</pubDate>
    </item>
    <item>
      <title>ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks</title>
      <link>http://arxiv.org/abs/2602.23285v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haohui Jia, Zheng Chen, Lingwei Zhu, Rikuto Kotoge, Jathurshan Pradeepkumar, Yasuko Matsubara, Jimeng Sun, Yasushi Sakurai&lt;/p&gt;&lt;p&gt;Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23285v1</guid>
      <pubDate>Thu, 26 Feb 2026 17:59:10 +0000</pubDate>
    </item>
    <item>
      <title>SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables</title>
      <link>http://arxiv.org/abs/2602.23286v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sungho Park, Jueun Kim, Wook-Shin Han&lt;/p&gt;&lt;p&gt;Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweigh&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23286v1</guid>
      <pubDate>Thu, 26 Feb 2026 17:59:51 +0000</pubDate>
    </item>
    <item>
      <title>Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</title>
      <link>http://arxiv.org/abs/2602.23296v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Quang-Huy Nguyen, Jiaqi Wang, Wei-Shinn Ku&lt;/p&gt;&lt;p&gt;Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23296v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:07:45 +0000</pubDate>
    </item>
    <item>
      <title>Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction</title>
      <link>http://arxiv.org/abs/2602.23312v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Rafael R. Baptista, André de Lima Salgado, Ricardo V. Godoy, Marcelo Becker, Thiago Boaventura, Gustavo J. G. Lahr&lt;/p&gt;&lt;p&gt;Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23312v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:20:26 +0000</pubDate>
    </item>
    <item>
      <title>Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction</title>
      <link>http://arxiv.org/abs/2602.23315v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sha Hu&lt;/p&gt;&lt;p&gt;An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23315v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:22:40 +0000</pubDate>
    </item>
    <item>
      <title>LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</title>
      <link>http://arxiv.org/abs/2602.23329v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus, Jason Hausenloy, Pedro Medeiros, Nathaniel Li, Aiden Kim, Yury Orlovskiy&lt;/p&gt;&lt;p&gt;Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23329v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:37:23 +0000</pubDate>
    </item>
    <item>
      <title>Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</title>
      <link>http://arxiv.org/abs/2602.23330v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts, Stefan Zohren&lt;/p&gt;&lt;p&gt;The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23330v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:37:36 +0000</pubDate>
    </item>
    <item>
      <title>Utilizing LLMs for Industrial Process Automation</title>
      <link>http://arxiv.org/abs/2602.23331v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Salim Fares&lt;/p&gt;&lt;p&gt;A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23331v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:38:00 +0000</pubDate>
    </item>
    <item>
      <title>Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators</title>
      <link>http://arxiv.org/abs/2602.23334v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuhao Liu, Salim Ullah, Akash Kumar&lt;/p&gt;&lt;p&gt;Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23334v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:40:02 +0000</pubDate>
    </item>
    <item>
      <title>Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</title>
      <link>http://arxiv.org/abs/2602.23335v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dany Haddad, Dan Bareket, Joseph Chee Chang, Jay DeYoung, Jena D. Hwang, Uri Katz, Mark Polak, Sangho Suh&lt;/p&gt;&lt;p&gt;AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23335v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:40:28 +0000</pubDate>
    </item>
    <item>
      <title>Differentiable Zero-One Loss via Hypersimplex Projections</title>
      <link>http://arxiv.org/abs/2602.23336v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Camilo Gomez, Pengyang Wang, Liansheng Tang&lt;/p&gt;&lt;p&gt;Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization f&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23336v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:41:31 +0000</pubDate>
    </item>
    <item>
      <title>Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</title>
      <link>http://arxiv.org/abs/2602.23341v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis, Felix Zhou, Ziyu Zhu&lt;/p&gt;&lt;p&gt;Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23341v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:47:06 +0000</pubDate>
    </item>
    <item>
      <title>FlashOptim: Optimizers for Memory Efficient Training</title>
      <link>http://arxiv.org/abs/2602.23349v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard, Davis Blalock&lt;/p&gt;&lt;p&gt;Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23349v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:52:22 +0000</pubDate>
    </item>
    <item>
      <title>SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</title>
      <link>http://arxiv.org/abs/2602.23353v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata&lt;/p&gt;&lt;p&gt;The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23353v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:55:06 +0000</pubDate>
    </item>
    <item>
      <title>SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2602.23359v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vaibhav Agrawal, Rishubh Parihar, Pradhaan Bhat, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu&lt;/p&gt;&lt;p&gt;We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23359v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:59:05 +0000</pubDate>
    </item>
    <item>
      <title>Model Agreement via Anchoring</title>
      <link>http://arxiv.org/abs/2602.23360v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Eric Eaton, Surbhi Goel, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Bela Sengupta, Jessica Sorrell&lt;/p&gt;&lt;p&gt;Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.23360v1</guid>
      <pubDate>Thu, 26 Feb 2026 18:59:32 +0000</pubDate>
    </item>
  </channel>
</rss>
