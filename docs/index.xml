<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 29 Jan 2026 04:08:16 +0000</lastBuildDate>
    <item>
      <title>Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction</title>
      <link>http://arxiv.org/abs/2601.20720v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Matej Halinkovic, Nina Masarykova, Alexey Vinel, Marek Galinski&lt;/p&gt;&lt;p&gt;End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20720v1</guid>
      <pubDate>Wed, 28 Jan 2026 15:53:32 +0000</pubDate>
    </item>
    <item>
      <title>QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks</title>
      <link>http://arxiv.org/abs/2601.20731v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mae Sosto, Delfina Sol Martinez Pandiani, Laura Hollink&lt;/p&gt;&lt;p&gt;This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20731v1</guid>
      <pubDate>Wed, 28 Jan 2026 16:06:04 +0000</pubDate>
    </item>
    <item>
      <title>SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning</title>
      <link>http://arxiv.org/abs/2601.20738v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dawit Kiros Redie, Reza Arablouei, Stefan Werner&lt;/p&gt;&lt;p&gt;Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20738v1</guid>
      <pubDate>Wed, 28 Jan 2026 16:10:49 +0000</pubDate>
    </item>
    <item>
      <title>HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs</title>
      <link>http://arxiv.org/abs/2601.20745v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Guoan Wang, Feiyu Wang, Zongwei Lv, Yikun Zong, Tong Yang&lt;/p&gt;&lt;p&gt;As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framewor&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20745v1</guid>
      <pubDate>Wed, 28 Jan 2026 16:22:42 +0000</pubDate>
    </item>
    <item>
      <title>REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence</title>
      <link>http://arxiv.org/abs/2601.20784v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zishen Wan, Che-Kai Liu, Jiayi Qian, Hanchen Yang, Arijit Raychowdhury, Tushar Krishna&lt;/p&gt;&lt;p&gt;Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arit&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20784v1</guid>
      <pubDate>Wed, 28 Jan 2026 17:17:21 +0000</pubDate>
    </item>
    <item>
      <title>FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models</title>
      <link>http://arxiv.org/abs/2601.20791v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haonan Zhong, Wei Song, Tingxu Han, Maurice Pagnucco, Jingling Xue, Yang Song&lt;/p&gt;&lt;p&gt;Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20791v1</guid>
      <pubDate>Wed, 28 Jan 2026 17:29:53 +0000</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning via Self-Distillation</title>
      <link>http://arxiv.org/abs/2601.20802v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld&lt;/p&gt;&lt;p&gt;Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20802v1</guid>
      <pubDate>Wed, 28 Jan 2026 17:45:12 +0000</pubDate>
    </item>
    <item>
      <title>GNN Explanations that do not Explain and How to find Them</title>
      <link>http://arxiv.org/abs/2601.20815v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Steve Azzolin, Stefano Teso, Bruno Lepri, Andrea Passerini, Sagar Malhotra&lt;/p&gt;&lt;p&gt;Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20815v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:05:17 +0000</pubDate>
    </item>
    <item>
      <title>Demystifying Prediction Powered Inference</title>
      <link>http://arxiv.org/abs/2601.20819v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yilin Song, Dan M. Kluger, Harsh Parikh, Tian Gu&lt;/p&gt;&lt;p&gt;Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20819v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:16:02 +0000</pubDate>
    </item>
    <item>
      <title>Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning</title>
      <link>http://arxiv.org/abs/2601.20829v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Minwu Kim, Safal Shrestha, Keith Ross&lt;/p&gt;&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20829v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:29:21 +0000</pubDate>
    </item>
    <item>
      <title>VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring</title>
      <link>http://arxiv.org/abs/2601.20830v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Waldyn G. Martinez&lt;/p&gt;&lt;p&gt;Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20830v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:30:48 +0000</pubDate>
    </item>
    <item>
      <title>MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents</title>
      <link>http://arxiv.org/abs/2601.20831v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vishnu Sashank Dorbala, Dinesh Manocha&lt;/p&gt;&lt;p&gt;Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20831v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:31:17 +0000</pubDate>
    </item>
    <item>
      <title>Open-Vocabulary Functional 3D Human-Scene Interaction Generation</title>
      <link>http://arxiv.org/abs/2601.20835v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang&lt;/p&gt;&lt;p&gt;Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20835v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:34:25 +0000</pubDate>
    </item>
    <item>
      <title>Reward Models Inherit Value Biases from Pretraining</title>
      <link>http://arxiv.org/abs/2601.20838v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska&lt;/p&gt;&lt;p&gt;Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20838v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:40:29 +0000</pubDate>
    </item>
    <item>
      <title>Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)</title>
      <link>http://arxiv.org/abs/2601.20843v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Saurav Prateek&lt;/p&gt;&lt;p&gt;This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runti&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20843v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:45:39 +0000</pubDate>
    </item>
    <item>
      <title>A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion</title>
      <link>http://arxiv.org/abs/2601.20847v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Willams de Lima Costa, Thifany Ketuli Silva de Souza, Jonas Ferreira Silva, Carlos Gabriel Bezerra Pereira, Bruno Reis Vila Nova, Leonardo Silvino Brito, Rafael Raider Leoni, Juliano Silva&lt;/p&gt;&lt;p&gt;Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20847v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:46:29 +0000</pubDate>
    </item>
    <item>
      <title>Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</title>
      <link>http://arxiv.org/abs/2601.20848v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Weixin Chen, Li Chen, Yuhan Zhao&lt;/p&gt;&lt;p&gt;Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20848v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:48:43 +0000</pubDate>
    </item>
    <item>
      <title>Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</title>
      <link>http://arxiv.org/abs/2601.20854v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares&lt;/p&gt;&lt;p&gt;Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20854v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:54:27 +0000</pubDate>
    </item>
    <item>
      <title>SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2601.20856v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri&lt;/p&gt;&lt;p&gt;Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20856v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:56:00 +0000</pubDate>
    </item>
    <item>
      <title>Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</title>
      <link>http://arxiv.org/abs/2601.20861v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli&lt;/p&gt;&lt;p&gt;One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.20861v1</guid>
      <pubDate>Wed, 28 Jan 2026 18:59:34 +0000</pubDate>
    </item>
  </channel>
</rss>
