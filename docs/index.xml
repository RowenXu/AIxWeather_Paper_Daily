<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 21 Jan 2026 03:45:28 +0000</lastBuildDate>
    <item>
      <title>Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models</title>
      <link>http://arxiv.org/abs/2601.13533v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Changshuo Zhang&lt;/p&gt;&lt;p&gt;Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's d&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13533v1</guid>
      <pubDate>Tue, 20 Jan 2026 02:32:39 +0000</pubDate>
    </item>
    <item>
      <title>MN-TSG:Continuous Time Series Generation with Irregular Observations</title>
      <link>http://arxiv.org/abs/2601.13534v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xu Zhang, Junwei Deng, Chang Xu, Hao Li, Jiang Bian&lt;/p&gt;&lt;p&gt;Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13534v1</guid>
      <pubDate>Tue, 20 Jan 2026 02:45:03 +0000</pubDate>
    </item>
    <item>
      <title>When Wording Steers the Evaluation: Framing Bias in LLM judges</title>
      <link>http://arxiv.org/abs/2601.13537v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yerin Hwang, Dongryeol Lee, Taegwan Kang, Minwoo Lee, Kyomin Jung&lt;/p&gt;&lt;p&gt;Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13537v1</guid>
      <pubDate>Tue, 20 Jan 2026 02:48:10 +0000</pubDate>
    </item>
    <item>
      <title>TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning</title>
      <link>http://arxiv.org/abs/2601.13545v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shirin Shahabi, Spencer Graham, Haruna Isah&lt;/p&gt;&lt;p&gt;Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to liv&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13545v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:11:47 +0000</pubDate>
    </item>
    <item>
      <title>ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution</title>
      <link>http://arxiv.org/abs/2601.13546v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hui Sun, Chang Xu, Haonan Xie, Hao Li, Yuhao Huang, Chuheng Zhang, Ming Jin, Xiaoguang Liu&lt;/p&gt;&lt;p&gt;LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13546v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:12:37 +0000</pubDate>
    </item>
    <item>
      <title>HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations</title>
      <link>http://arxiv.org/abs/2601.13547v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yujia Hu, Roy Ka-Wei Lee&lt;/p&gt;&lt;p&gt;Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13547v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:13:07 +0000</pubDate>
    </item>
    <item>
      <title>Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis</title>
      <link>http://arxiv.org/abs/2601.13558v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mehrab Beikzadeh, Chenglin Hong, Cory J Cascalheira, Callisto Boka, Majid Sarrafzadeh, Ian W Holloway&lt;/p&gt;&lt;p&gt;Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13558v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:28:50 +0000</pubDate>
    </item>
    <item>
      <title>AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent</title>
      <link>http://arxiv.org/abs/2601.13559v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sun Hui, Ding Yanfeng, Huidong Ma, Chang Xu, Keyan Jin, Lizheng Zu, Cheng Zhong, xiaoguang Liu&lt;/p&gt;&lt;p&gt;Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13559v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:29:45 +0000</pubDate>
    </item>
    <item>
      <title>Reasoning is a Modality</title>
      <link>http://arxiv.org/abs/2601.13562v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhiguang Liu, Yi Shang&lt;/p&gt;&lt;p&gt;The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13562v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:37:17 +0000</pubDate>
    </item>
    <item>
      <title>ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits</title>
      <link>http://arxiv.org/abs/2601.13563v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Aryan Karmore&lt;/p&gt;&lt;p&gt;Linear memory scaling stores $N$ independent expert weight matrices requiring $\mathcal{O}(N \cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13563v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:39:33 +0000</pubDate>
    </item>
    <item>
      <title>Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework</title>
      <link>http://arxiv.org/abs/2601.13564v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao&lt;/p&gt;&lt;p&gt;Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13564v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:41:02 +0000</pubDate>
    </item>
    <item>
      <title>Self-Improvement as Coherence Optimization: A Theoretical Account</title>
      <link>http://arxiv.org/abs/2601.13566v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tianyi Qiu, Ahmed Hani Ismail, Zhonghao He, Shi Feng&lt;/p&gt;&lt;p&gt;Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13566v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:50:02 +0000</pubDate>
    </item>
    <item>
      <title>GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds</title>
      <link>http://arxiv.org/abs/2601.13570v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tingting Dan, Jiaqi Ding, Guorong Wu&lt;/p&gt;&lt;p&gt;State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieved powerful fits to functional neuroimaging data. However, most existing approaches still view the brain as a set of loosely connected regions or impose oversimplified network priors, falling short of a truly holistic and self-organized dynamical system perspective&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13570v1</guid>
      <pubDate>Tue, 20 Jan 2026 03:56:06 +0000</pubDate>
    </item>
    <item>
      <title>Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models</title>
      <link>http://arxiv.org/abs/2601.13580v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ahmad Al-Zuraiqi&lt;/p&gt;&lt;p&gt;We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets ("donor organs") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13580v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:10:57 +0000</pubDate>
    </item>
    <item>
      <title>SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System</title>
      <link>http://arxiv.org/abs/2601.13581v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Heedou Kim, Changsik Kim, Sanghwa Shin, Jaewoo Kang&lt;/p&gt;&lt;p&gt;Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13581v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:11:00 +0000</pubDate>
    </item>
    <item>
      <title>TREX: Tokenizer Regression for Optimal Data Mixture</title>
      <link>http://arxiv.org/abs/2601.13588v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Inho Won, Hangyeol Yoo, Minkyung Cho, Jungyeul Park, Hoyun Song, KyungTae Lim&lt;/p&gt;&lt;p&gt;Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13588v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:41:09 +0000</pubDate>
    </item>
    <item>
      <title>Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification</title>
      <link>http://arxiv.org/abs/2601.13589v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; HyeYoung Lee&lt;/p&gt;&lt;p&gt;This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a R&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13589v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:42:03 +0000</pubDate>
    </item>
    <item>
      <title>Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions</title>
      <link>http://arxiv.org/abs/2601.13590v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Fan Huang, Haewoon Kwak, Jisun An&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13590v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:43:55 +0000</pubDate>
    </item>
    <item>
      <title>DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems</title>
      <link>http://arxiv.org/abs/2601.13591v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Maojun Sun, Yifei Xie, Yue Wu, Ruijian Han, Binyan Jiang, Defeng Sun, Yancheng Yuan, Jian Huang&lt;/p&gt;&lt;p&gt;Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13591v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:44:36 +0000</pubDate>
    </item>
    <item>
      <title>Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments</title>
      <link>http://arxiv.org/abs/2601.13592v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hao Jing, Sa Xiao, Haoyu Li, Huadong Xiao, Wei Xue&lt;/p&gt;&lt;p&gt;Radiation is typically the most time-consuming physical process in numerical models. One solution is to use machine learning methods to simulate the radiation process to improve computational efficiency. From an operational standpoint, this study investigates critical limitations inherent to hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, with a specific focus on two fundamental bottlenecks: coupling compatibility and long-term integration stability&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.13592v1</guid>
      <pubDate>Tue, 20 Jan 2026 04:45:45 +0000</pubDate>
    </item>
  </channel>
</rss>
