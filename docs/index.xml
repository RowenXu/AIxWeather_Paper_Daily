<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 31 Jan 2026 04:06:53 +0000</lastBuildDate>
    <item>
      <title>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</title>
      <link>http://arxiv.org/abs/2601.22054v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen&lt;/p&gt;&lt;p&gt;Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal in&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22054v1</guid>
      <pubDate>Thu, 29 Jan 2026 17:52:41 +0000</pubDate>
    </item>
    <item>
      <title>Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models</title>
      <link>http://arxiv.org/abs/2601.22057v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Archer Wang, Emile Anand, Yilun Du, Marin Soljačić&lt;/p&gt;&lt;p&gt;Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22057v1</guid>
      <pubDate>Thu, 29 Jan 2026 17:57:06 +0000</pubDate>
    </item>
    <item>
      <title>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2601.22060v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen&lt;/p&gt;&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the questi&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22060v1</guid>
      <pubDate>Thu, 29 Jan 2026 17:58:40 +0000</pubDate>
    </item>
    <item>
      <title>Latent Adversarial Regularization for Offline Preference Optimization</title>
      <link>http://arxiv.org/abs/2601.22083v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Enyi Jiang, Yibo Jacky Zhang, Yinglun Xu, Andreas Haupt, Nancy Amato, Sanmi Koyejo&lt;/p&gt;&lt;p&gt;Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22083v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:21:57 +0000</pubDate>
    </item>
    <item>
      <title>Investigating Associational Biases in Inter-Model Communication of Large Generative Models</title>
      <link>http://arxiv.org/abs/2601.22093v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Fethiye Irmak Dogan, Yuval Weiss, Kajal Patel, Jiaee Cheong, Hatice Gunes&lt;/p&gt;&lt;p&gt;Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition a&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22093v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:29:55 +0000</pubDate>
    </item>
    <item>
      <title>ECO: Quantized Training without Full-Precision Master Weights</title>
      <link>http://arxiv.org/abs/2601.22101v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mahdi Nikdan, Amir Zandieh, Dan Alistarh, Vahab Mirrokni&lt;/p&gt;&lt;p&gt;Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22101v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:35:01 +0000</pubDate>
    </item>
    <item>
      <title>Value-Based Pre-Training with Downstream Feedback</title>
      <link>http://arxiv.org/abs/2601.22108v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shuqi Ke, Giulia Fanti&lt;/p&gt;&lt;p&gt;Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22108v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:38:09 +0000</pubDate>
    </item>
    <item>
      <title>Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment</title>
      <link>http://arxiv.org/abs/2601.22111v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Abdullah Tasim, Wei Sun&lt;/p&gt;&lt;p&gt;Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22111v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:40:32 +0000</pubDate>
    </item>
    <item>
      <title>SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</title>
      <link>http://arxiv.org/abs/2601.22114v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi&lt;/p&gt;&lt;p&gt;Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22114v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:41:52 +0000</pubDate>
    </item>
    <item>
      <title>Defining Operational Conditions for Safety-Critical AI-Based Systems from Data</title>
      <link>http://arxiv.org/abs/2601.22118v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Johann Christensen, Elena Hoemann, Frank Köster, Sven Hallerbach&lt;/p&gt;&lt;p&gt;Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22118v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:46:02 +0000</pubDate>
    </item>
    <item>
      <title>The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR</title>
      <link>http://arxiv.org/abs/2601.22128v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Irsyad Adam, Zekai Chen, David Laprade, Shaun Porwal, David Laub, Erik Reinertsen, Arda Pekis, Kevin Brown&lt;/p&gt;&lt;p&gt;Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22128v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:49:37 +0000</pubDate>
    </item>
    <item>
      <title>SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</title>
      <link>http://arxiv.org/abs/2601.22129v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yifeng Ding, Lingming Zhang&lt;/p&gt;&lt;p&gt;Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22129v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:50:29 +0000</pubDate>
    </item>
    <item>
      <title>World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems</title>
      <link>http://arxiv.org/abs/2601.22130v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lakshya Gupta, Litao Li, Yizhe Liu, Sriram Ganapathi Subramanian, Kaheer Suleman, Zichen Zhang, Haoye Lu, Sumit Pasupalak&lt;/p&gt;&lt;p&gt;Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22130v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:51:54 +0000</pubDate>
    </item>
    <item>
      <title>StepShield: When, Not Whether to Intervene on Rogue Agents</title>
      <link>http://arxiv.org/abs/2601.22136v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli&lt;/p&gt;&lt;p&gt;Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22136v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:55:46 +0000</pubDate>
    </item>
    <item>
      <title>Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers</title>
      <link>http://arxiv.org/abs/2601.22139v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang&lt;/p&gt;&lt;p&gt;Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22139v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:56:12 +0000</pubDate>
    </item>
    <item>
      <title>Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</title>
      <link>http://arxiv.org/abs/2601.22141v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Grzegorz Stefanski, Alberto Presta, Michal Byra&lt;/p&gt;&lt;p&gt;In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22141v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:56:41 +0000</pubDate>
    </item>
    <item>
      <title>DynaWeb: Model-Based Reinforcement Learning of Web Agents</title>
      <link>http://arxiv.org/abs/2601.22149v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang&lt;/p&gt;&lt;p&gt;The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22149v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:59:07 +0000</pubDate>
    </item>
    <item>
      <title>Exploring Reasoning Reward Model for Agents</title>
      <link>http://arxiv.org/abs/2601.22154v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei&lt;/p&gt;&lt;p&gt;Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22154v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:59:52 +0000</pubDate>
    </item>
    <item>
      <title>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</title>
      <link>http://arxiv.org/abs/2601.22156v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han&lt;/p&gt;&lt;p&gt;Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models als&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22156v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:59:53 +0000</pubDate>
    </item>
    <item>
      <title>RedSage: A Cybersecurity Generalist LLM</title>
      <link>http://arxiv.org/abs/2601.22159v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani&lt;/p&gt;&lt;p&gt;Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.22159v1</guid>
      <pubDate>Thu, 29 Jan 2026 18:59:57 +0000</pubDate>
    </item>
  </channel>
</rss>
