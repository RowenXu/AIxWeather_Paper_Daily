<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 06 Nov 2025 03:15:00 +0000</lastBuildDate>
    <item>
      <title>The Collaboration Gap</title>
      <link>http://arxiv.org/abs/2511.02687v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tim R. Davidson, Adam Fourney, Saleema Amershi, Robert West, Eric Horvitz, Ece Kamar&lt;/p&gt;&lt;p&gt;The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02687v1</guid>
      <pubDate>Tue, 04 Nov 2025 16:10:57 +0000</pubDate>
    </item>
    <item>
      <title>LLEXICORP: End-user Explainability of Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02720v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vojtěch Kůr, Adam Bajger, Adam Kukučka, Marek Hradil, Vít Musil, Tomáš Brázdil&lt;/p&gt;&lt;p&gt;Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02720v1</guid>
      <pubDate>Tue, 04 Nov 2025 16:44:45 +0000</pubDate>
    </item>
    <item>
      <title>CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents</title>
      <link>http://arxiv.org/abs/2511.02734v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jiayu Liu, Cheng Qian, Zhaochen Su, Qing Zong, Shijue Huang, Bingxiang He, Yi R. Fung&lt;/p&gt;&lt;p&gt;Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02734v1</guid>
      <pubDate>Tue, 04 Nov 2025 16:58:29 +0000</pubDate>
    </item>
    <item>
      <title>Using Span Queries to Optimize for Cache and Attention Locality</title>
      <link>http://arxiv.org/abs/2511.02749v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Paul Castro, Nick Mitchell, Nathan Ordonez, Thomas Parnell, Mudhakar Srivatsa, Antoni Viros i Martin&lt;/p&gt;&lt;p&gt;Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02749v1</guid>
      <pubDate>Tue, 04 Nov 2025 17:22:49 +0000</pubDate>
    </item>
    <item>
      <title>AI Diffusion in Low Resource Language Countries</title>
      <link>http://arxiv.org/abs/2511.02752v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Amit Misra, Syed Waqas Zamir, Wassim Hamidouche, Inbal Becker-Reshef, Juan Lavista Ferres&lt;/p&gt;&lt;p&gt;Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02752v1</guid>
      <pubDate>Tue, 04 Nov 2025 17:31:39 +0000</pubDate>
    </item>
    <item>
      <title>ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models</title>
      <link>http://arxiv.org/abs/2511.02757v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lejs Deen Behric, Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil&lt;/p&gt;&lt;p&gt;Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy for finetuning large language models (LLMs) because it eliminates the memory overhead of backpropagation. However, it converges slowly due to the inherent curse of dimensionality when searching for descent directions in the high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a novel zeroth-order optimizer that accelerates convergence by adaptive directional sampling&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02757v1</guid>
      <pubDate>Tue, 04 Nov 2025 17:35:52 +0000</pubDate>
    </item>
    <item>
      <title>LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer</title>
      <link>http://arxiv.org/abs/2511.02759v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Julius Fiedler, Carsten Knoll, Klaus Röbenack&lt;/p&gt;&lt;p&gt;The rapid growth of research output in control engineering calls for new approaches to structure and formalize domain knowledge. This paper briefly describes an LLM-supported method for semi-automated generation of formal knowledge representations that combine human readability with machine interpretability and increased expressiveness. Based on the Imperative Representation of Knowledge (PyIRK) framework, we demonstrate how language models can assist in transforming natural-language descriptions and mathematical definitions (available as LaTeX source code) into a formalized knowledge graph&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02759v1</guid>
      <pubDate>Tue, 04 Nov 2025 17:36:57 +0000</pubDate>
    </item>
    <item>
      <title>STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation</title>
      <link>http://arxiv.org/abs/2511.02769v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Bum Chul Kwon, Ben Shapira, Moshiko Raboh, Shreyans Sethi, Shruti Murarka, Joseph A Morrone, Jianying Hu, Parthasarathy Suryanarayanan&lt;/p&gt;&lt;p&gt;The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable lat&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02769v1</guid>
      <pubDate>Tue, 04 Nov 2025 17:56:00 +0000</pubDate>
    </item>
    <item>
      <title>Measuring AI Diffusion: A Population-Normalized Metric for Tracking Global AI Usage</title>
      <link>http://arxiv.org/abs/2511.02781v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Amit Misra, Jane Wang, Scott McCullers, Kevin White, Juan Lavista Ferres&lt;/p&gt;&lt;p&gt;Measuring global AI diffusion remains challenging due to a lack of population-normalized, cross-country usage data. We introduce AI User Share, a novel indicator that estimates the share of each country's working-age population actively using AI tools. Built from anonymized Microsoft telemetry and adjusted for device access and mobile scaling, this metric spans 147 economies and provides consistent, real-time insight into global AI diffusion&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02781v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:03:51 +0000</pubDate>
    </item>
    <item>
      <title>When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning</title>
      <link>http://arxiv.org/abs/2511.02794v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang&lt;/p&gt;&lt;p&gt;Despite rapid growth in multimodal large language models (MLLMs), their reasoning traces remain opaque: it is often unclear which modality drives a prediction, how conflicts are resolved, or when one stream dominates. In this paper, we introduce modality sabotage, a diagnostic failure mode in which a high-confidence unimodal error overrides other evidence and misleads the fused result. To analyze such dynamics, we propose a lightweight, model-agnostic evaluation layer that treats each modality as an agent, producing candidate labels and a brief self-assessment used for auditing&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02794v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:20:13 +0000</pubDate>
    </item>
    <item>
      <title>Intercomparison of a High-Resolution Regional Climate Model Ensemble for Catchment-Scale Water Cycle Processes under Human Influence</title>
      <link>http://arxiv.org/abs/2511.02799v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; J. L. Roque, F. Da Silva Lopes, J. A. Giles, B. D. Gutknecht, B. Schalge, Y. Zhang, M. Ferro, P. Friederichs&lt;/p&gt;&lt;p&gt;Understanding regional hydroclimatic variability and its drivers is essential for anticipating the impacts of climate change on water resources and sustainability. Yet, considerable uncertainty remains in the simulation of the coupled land atmosphere water and energy cycles, largely due to structural model limitations, simplified process representations, and insufficient spatial resolution. Within the framework of the Collaborative Research Center 1502 DETECT, this study presents a coordinated intercomparison of regional climate model simulations designed for water cycle process analysis over &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02799v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:23:23 +0000</pubDate>
    </item>
    <item>
      <title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02802v2</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu&lt;/p&gt;&lt;p&gt;Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02802v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:36:30 +0000</pubDate>
    </item>
    <item>
      <title>MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2511.02805v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang&lt;/p&gt;&lt;p&gt;Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02805v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:27:39 +0000</pubDate>
    </item>
    <item>
      <title>Assessing win strength in MLB win prediction models</title>
      <link>http://arxiv.org/abs/2511.02815v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Morgan Allen, Paul Savala&lt;/p&gt;&lt;p&gt;In Major League Baseball, strategy and planning are major factors in determining the outcome of a game. Previous studies have aided this by building machine learning models for predicting the winning team of any given game. We extend this work by training a comprehensive set of machine learning models using a common dataset&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02815v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:40:10 +0000</pubDate>
    </item>
    <item>
      <title>Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities</title>
      <link>http://arxiv.org/abs/2511.02817v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley&lt;/p&gt;&lt;p&gt;As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02817v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:42:12 +0000</pubDate>
    </item>
    <item>
      <title>Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title>
      <link>http://arxiv.org/abs/2511.02818v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu&lt;/p&gt;&lt;p&gt;Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02818v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:43:44 +0000</pubDate>
    </item>
    <item>
      <title>Optimizing AI Agent Attacks With Synthetic Data</title>
      <link>http://arxiv.org/abs/2511.02823v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, Joe Benton&lt;/p&gt;&lt;p&gt;As AI deployments become more complex and high-stakes, it becomes increasingly important to be able to estimate their risk. AI control is one framework for doing so. However, good control evaluations require eliciting strong attack policies&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02823v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:48:56 +0000</pubDate>
    </item>
    <item>
      <title>Kosmos: An AI Scientist for Autonomous Discovery</title>
      <link>http://arxiv.org/abs/2511.02824v2</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ludovico Mitchener, Angela Yiu, Benjamin Chang, Mathieu Bourdenx, Tyler Nadolski, Arvis Sulovari, Eric C. Landsness, Daniel L. Barabasi&lt;/p&gt;&lt;p&gt;Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02824v2</guid>
      <pubDate>Wed, 05 Nov 2025 18:26:43 +0000</pubDate>
    </item>
    <item>
      <title>Neurosymbolic Deep Learning Semantics</title>
      <link>http://arxiv.org/abs/2511.02825v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Artur d'Avila Garcez, Simon Odense&lt;/p&gt;&lt;p&gt;Artificial Intelligence (AI) is a powerful new language of science as evidenced by recent Nobel Prizes in chemistry and physics that recognized contributions to AI applied to those areas. Yet, this new language lacks semantics, which makes AI's scientific discoveries unsatisfactory at best. With the purpose of uncovering new facts but also improving our understanding of the world, AI-based science requires formalization through a framework capable of translating insight into comprehensible scientific knowledge&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02825v1</guid>
      <pubDate>Tue, 04 Nov 2025 18:51:04 +0000</pubDate>
    </item>
    <item>
      <title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
      <link>http://arxiv.org/abs/2511.02834v2</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh&lt;/p&gt;&lt;p&gt;Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2511.02834v2</guid>
      <pubDate>Wed, 05 Nov 2025 05:50:54 +0000</pubDate>
    </item>
  </channel>
</rss>
