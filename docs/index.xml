<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 05 Feb 2026 04:16:16 +0000</lastBuildDate>
    <item>
      <title>Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation</title>
      <link>http://arxiv.org/abs/2602.04785v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Congjing Zhang, Ryan Feng Lin, Ruoxuan Bao, Shuai Huang&lt;/p&gt;&lt;p&gt;While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data qu&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04785v1</guid>
      <pubDate>Wed, 04 Feb 2026 17:34:41 +0000</pubDate>
    </item>
    <item>
      <title>Maximum-Volume Nonnegative Matrix Factorization</title>
      <link>http://arxiv.org/abs/2602.04795v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Olivier Vu Thanh, Nicolas Gillis&lt;/p&gt;&lt;p&gt;Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04795v1</guid>
      <pubDate>Wed, 04 Feb 2026 17:43:25 +0000</pubDate>
    </item>
    <item>
      <title>Score-Based Change-Point Detection and Region Localization for Spatio-Temporal Point Processes</title>
      <link>http://arxiv.org/abs/2602.04798v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Wenbin Zhou, Liyan Xie, Shixiang Zhu&lt;/p&gt;&lt;p&gt;We study sequential change-point detection for spatio-temporal point processes, where actionable detection requires not only identifying when a distributional change occurs but also localizing where it manifests in space. While classical quickest change detection methods provide strong guarantees on detection delay and false-alarm rates, existing approaches for point-process data predominantly focus on temporal changes and do not explicitly infer affected spatial regions. We propose a likelihood-free, score-based detection framework that jointly estimates the change time and the change region &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04798v1</guid>
      <pubDate>Wed, 04 Feb 2026 17:44:41 +0000</pubDate>
    </item>
    <item>
      <title>Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging</title>
      <link>http://arxiv.org/abs/2602.04805v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu&lt;/p&gt;&lt;p&gt;The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04805v1</guid>
      <pubDate>Wed, 04 Feb 2026 17:52:17 +0000</pubDate>
    </item>
    <item>
      <title>SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization</title>
      <link>http://arxiv.org/abs/2602.04811v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jiarui Yuan, Tailin Jin, Weize Chen, Zeyuan Liu, Zhiyuan Liu, Maosong Sun&lt;/p&gt;&lt;p&gt;True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized id&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04811v1</guid>
      <pubDate>Wed, 04 Feb 2026 17:58:32 +0000</pubDate>
    </item>
    <item>
      <title>Agentic AI in Healthcare &amp; Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents</title>
      <link>http://arxiv.org/abs/2602.04813v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shubham Vatsal, Harsh Dubey, Aditi Singh&lt;/p&gt;&lt;p&gt;Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04813v1</guid>
      <pubDate>Wed, 04 Feb 2026 17:59:14 +0000</pubDate>
    </item>
    <item>
      <title>Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization</title>
      <link>http://arxiv.org/abs/2602.04820v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Farzia Hossain, Samanta Ghosh, Shahida Begum, B. M. Shahria Alam, Mohammad Tahmid Noor, Md Parvez Mia, Nishat Tasnim Niloy&lt;/p&gt;&lt;p&gt;Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04820v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:08:13 +0000</pubDate>
    </item>
    <item>
      <title>Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2602.04821v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Joydeep Chandra, Satyam Kumar Navneet, Aleksandr Algazinov, Yong Zhang&lt;/p&gt;&lt;p&gt;Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals vi&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04821v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:10:59 +0000</pubDate>
    </item>
    <item>
      <title>Are AI Capabilities Increasing Exponentially? A Competing Hypothesis</title>
      <link>http://arxiv.org/abs/2602.04836v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haosen Ge, Hamsa Bastani, Osbert Bastani&lt;/p&gt;&lt;p&gt;Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation &amp; Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04836v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:28:49 +0000</pubDate>
    </item>
    <item>
      <title>Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing</title>
      <link>http://arxiv.org/abs/2602.04837v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhaotian Weng, Antonis Antoniades, Deepak Nathani, Zhen Zhang, Xiao Pu, Xin Eric Wang&lt;/p&gt;&lt;p&gt;Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utili&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04837v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:29:36 +0000</pubDate>
    </item>
    <item>
      <title>Fluid Representations in Reasoning Models</title>
      <link>http://arxiv.org/abs/2602.04843v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin&lt;/p&gt;&lt;p&gt;Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04843v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:34:50 +0000</pubDate>
    </item>
    <item>
      <title>El Agente Estructural: An Artificially Intelligent Molecular Editor</title>
      <link>http://arxiv.org/abs/2602.04849v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Changhyeok Choi, Yunheng Zou, Marcel Müller, Han Hao, Yeonghun Kang, Juan B. Pérez-Sánchez, Ignacio Gustin, Hanyong Xu&lt;/p&gt;&lt;p&gt;We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04849v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:38:48 +0000</pubDate>
    </item>
    <item>
      <title>El Agente Quntur: A research collaborator agent for quantum chemistry</title>
      <link>http://arxiv.org/abs/2602.04850v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Juan B. Pérez-Sánchez, Yunheng Zou, Jorge A. Campos-Gonzalez-Angulo, Marcel Müller, Ignacio Gustin, Andrew Wang, Han Hao, Tsz Wai Ko&lt;/p&gt;&lt;p&gt;Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04850v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:38:50 +0000</pubDate>
    </item>
    <item>
      <title>From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures</title>
      <link>http://arxiv.org/abs/2602.04861v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ryan Liu, Eric Qu, Tobias Kreiman, Samuel M. Blau, Aditi S. Krishnapriyan&lt;/p&gt;&lt;p&gt;Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04861v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:50:10 +0000</pubDate>
    </item>
    <item>
      <title>Subliminal Effects in Your Data: A General Mechanism via Log-Linearity</title>
      <link>http://arxiv.org/abs/2602.04863v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ishaq Aden-Ali, Noah Golowich, Allen Liu, Abhishek Shetty, Ankur Moitra, Nika Haghtalab&lt;/p&gt;&lt;p&gt;Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent wor&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04863v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:50:46 +0000</pubDate>
    </item>
    <item>
      <title>CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation</title>
      <link>http://arxiv.org/abs/2602.04868v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yannick Denker, Alexander Gepperth&lt;/p&gt;&lt;p&gt;Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04868v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:54:26 +0000</pubDate>
    </item>
    <item>
      <title>Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning</title>
      <link>http://arxiv.org/abs/2602.04872v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Nicholas Barnfield, Subhabrata Sen, Pragya Sur&lt;/p&gt;&lt;p&gt;Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04872v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:57:30 +0000</pubDate>
    </item>
    <item>
      <title>Rethinking the Trust Region in LLM Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2602.04879v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee&lt;/p&gt;&lt;p&gt;Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04879v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:59:04 +0000</pubDate>
    </item>
    <item>
      <title>Contrastive Continual Learning for Model Adaptability in Internet of Things</title>
      <link>http://arxiv.org/abs/2602.04881v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ajesh Koyatan Chathoth&lt;/p&gt;&lt;p&gt;Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04881v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:59:14 +0000</pubDate>
    </item>
    <item>
      <title>Protein Autoregressive Modeling via Multiscale Structure Generation</title>
      <link>http://arxiv.org/abs/2602.04883v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu&lt;/p&gt;&lt;p&gt;We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces condi&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.04883v1</guid>
      <pubDate>Wed, 04 Feb 2026 18:59:49 +0000</pubDate>
    </item>
  </channel>
</rss>
