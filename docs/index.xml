<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 18 Dec 2025 03:21:51 +0000</lastBuildDate>
    <item>
      <title>How Smoothing is N-simplicial Attention?</title>
      <link>http://arxiv.org/abs/2512.15600v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Alexandre Dussolle, Pietro Liò&lt;/p&gt;&lt;p&gt;Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15600v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:10:57 +0000</pubDate>
    </item>
    <item>
      <title>Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</title>
      <link>http://arxiv.org/abs/2512.15605v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mathieu Blondel, Michael E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet&lt;/p&gt;&lt;p&gt;Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15605v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:14:26 +0000</pubDate>
    </item>
    <item>
      <title>Evaluating Metrics for Safety with LLM-as-Judges</title>
      <link>http://arxiv.org/abs/2512.15617v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kester Clegg, Richard Hawkins, Ibrahim Habli, Tom Lawton&lt;/p&gt;&lt;p&gt;LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15617v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:24:49 +0000</pubDate>
    </item>
    <item>
      <title>How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</title>
      <link>http://arxiv.org/abs/2512.15634v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Darshita Rathore, Vineet Kumar, Chetna Bansal, Anindya Moitra&lt;/p&gt;&lt;p&gt;Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&amp;A tasks and generalisation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15634v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:44:09 +0000</pubDate>
    </item>
    <item>
      <title>IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</title>
      <link>http://arxiv.org/abs/2512.15635v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao&lt;/p&gt;&lt;p&gt;We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15635v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:47:18 +0000</pubDate>
    </item>
    <item>
      <title>Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation</title>
      <link>http://arxiv.org/abs/2512.15643v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jairo Fúquene-Patiño&lt;/p&gt;&lt;p&gt;In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15643v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:51:21 +0000</pubDate>
    </item>
    <item>
      <title>VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
      <link>http://arxiv.org/abs/2512.15649v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang&lt;/p&gt;&lt;p&gt;The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15649v1</guid>
      <pubDate>Wed, 17 Dec 2025 17:58:35 +0000</pubDate>
    </item>
    <item>
      <title>A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition</title>
      <link>http://arxiv.org/abs/2512.15650v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Stephen Tivenan, Indranil Sahoo, Yanjun Qian&lt;/p&gt;&lt;p&gt;Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and as&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15650v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:02:40 +0000</pubDate>
    </item>
    <item>
      <title>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</title>
      <link>http://arxiv.org/abs/2512.15658v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan&lt;/p&gt;&lt;p&gt;Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15658v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:11:29 +0000</pubDate>
    </item>
    <item>
      <title>Prospects for quantum advantage in machine learning from the representability of functions</title>
      <link>http://arxiv.org/abs/2512.15661v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sergi Masot-Llima, Elies Gil-Fuster, Carlos Bravo-Prieto, Jens Eisert, and Tommaso Guaita&lt;/p&gt;&lt;p&gt;Demonstrating quantum advantage in machine learning tasks requires navigating a complex landscape of proposed models and algorithms. To bring clarity to this search, we introduce a framework that connects the structure of parametrized quantum circuits to the mathematical nature of the functions they can actually learn. Within this framework, we show how fundamental properties, like circuit depth and non-Clifford gate count, directly determine whether a model's output leads to efficient classical simulation or surrogation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15661v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:14:59 +0000</pubDate>
    </item>
    <item>
      <title>Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</title>
      <link>http://arxiv.org/abs/2512.15662v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan LU&lt;/p&gt;&lt;p&gt;Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15662v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:15:17 +0000</pubDate>
    </item>
    <item>
      <title>Explaining the Reasoning of Large Language Models Using Attribution Graphs</title>
      <link>http://arxiv.org/abs/2512.15663v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chase Walker, Rickard Ewetz&lt;/p&gt;&lt;p&gt;Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15663v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:15:26 +0000</pubDate>
    </item>
    <item>
      <title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title>
      <link>http://arxiv.org/abs/2512.15674v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Adam Karvonen, James Chua, Clément Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma&lt;/p&gt;&lt;p&gt;Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15674v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:26:28 +0000</pubDate>
    </item>
    <item>
      <title>High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations</title>
      <link>http://arxiv.org/abs/2512.15684v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Victor Léger, Florent Chatelain&lt;/p&gt;&lt;p&gt;Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15684v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:38:01 +0000</pubDate>
    </item>
    <item>
      <title>Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</title>
      <link>http://arxiv.org/abs/2512.15687v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu&lt;/p&gt;&lt;p&gt;Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15687v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:44:45 +0000</pubDate>
    </item>
    <item>
      <title>BashArena: A Control Setting for Highly Privileged AI Agents</title>
      <link>http://arxiv.org/abs/2512.15688v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Adam Kaufman, James Lucassen, Tyler Tracy, Cody Rushing, Aryan Bhatt&lt;/p&gt;&lt;p&gt;Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15688v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:45:25 +0000</pubDate>
    </item>
    <item>
      <title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
      <link>http://arxiv.org/abs/2512.15692v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava&lt;/p&gt;&lt;p&gt;Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15692v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:47:31 +0000</pubDate>
    </item>
    <item>
      <title>Artism: AI-Driven Dual-Engine System for Art Generation and Critique</title>
      <link>http://arxiv.org/abs/2512.15710v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shuai Liu, Yiqing Tian, Yang Chen, Mar Canet Sola&lt;/p&gt;&lt;p&gt;This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15710v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:58:42 +0000</pubDate>
    </item>
    <item>
      <title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title>
      <link>http://arxiv.org/abs/2512.15712v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt&lt;/p&gt;&lt;p&gt;Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15712v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:59:48 +0000</pubDate>
    </item>
    <item>
      <title>Spatia: Video Generation with Updatable Spatial Memory</title>
      <link>http://arxiv.org/abs/2512.15716v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu&lt;/p&gt;&lt;p&gt;Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.15716v1</guid>
      <pubDate>Wed, 17 Dec 2025 18:59:59 +0000</pubDate>
    </item>
  </channel>
</rss>
