<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 20 Feb 2026 04:16:33 +0000</lastBuildDate>
    <item>
      <title>ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment</title>
      <link>http://arxiv.org/abs/2602.17560v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hongjue Zhao, Haosen Sun, Jiangtao Kong, Xiaochang Li, Qineng Wang, Liwei Jiang, Qi Zhu, Tarek Abdelzaher&lt;/p&gt;&lt;p&gt;Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activatio&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17560v1</guid>
      <pubDate>Thu, 19 Feb 2026 17:13:44 +0000</pubDate>
    </item>
    <item>
      <title>Optimal Unconstrained Self-Distillation in Ridge Regression: Strict Improvements, Precise Asymptotics, and One-Shot Tuning</title>
      <link>http://arxiv.org/abs/2602.17565v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hien Dang, Pratik Patil, Alessandro Rinaldo&lt;/p&gt;&lt;p&gt;Self-distillation (SD) is the process of retraining a student on a mixture of ground-truth labels and the teacher's own predictions using the same architecture and training data. Although SD has been empirically shown to often improve generalization, its formal guarantees remain limited. We study SD for ridge regression in unconstrained setting in which the mixing weight $ξ$ may be outside the unit interval&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17565v1</guid>
      <pubDate>Thu, 19 Feb 2026 17:21:15 +0000</pubDate>
    </item>
    <item>
      <title>A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN</title>
      <link>http://arxiv.org/abs/2602.17566v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Asif Hasan Chowdhury, Md. Fahim Islam, M Ragib Anjum Riad, Faiyaz Bin Hashem, Md Tanzim Reza, Md. Golam Rabiul Alam&lt;/p&gt;&lt;p&gt;The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medi&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17566v1</guid>
      <pubDate>Thu, 19 Feb 2026 17:22:50 +0000</pubDate>
    </item>
    <item>
      <title>Be Wary of Your Time Series Preprocessing</title>
      <link>http://arxiv.org/abs/2602.17568v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sofiane Ennadir, Tianze Wang, Oleg Smirnov, Sahar Asadi, Lele Cao&lt;/p&gt;&lt;p&gt;Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representati&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17568v1</guid>
      <pubDate>Thu, 19 Feb 2026 17:23:56 +0000</pubDate>
    </item>
    <item>
      <title>Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space</title>
      <link>http://arxiv.org/abs/2602.17586v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Antonio Guillen-Perez&lt;/p&gt;&lt;p&gt;Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank sp&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17586v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:10:16 +0000</pubDate>
    </item>
    <item>
      <title>Asymptotically Optimal Sequential Testing with Markovian Data</title>
      <link>http://arxiv.org/abs/2602.17587v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Alhad Sethi, Kavali Sofia Sagar, Shubhada Agrawal, Debabrota Basu, P. N. Karthik&lt;/p&gt;&lt;p&gt;We study one-sided and $α$-correct sequential hypothesis testing for data generated by an ergodic Markov chain. The null hypothesis is that the unknown transition matrix belongs to a prescribed set $P$ of stochastic matrices, and the alternative corresponds to a disjoint set $Q$. We establish a tight non-asymptotic instance-dependent lower bound on the expected stopping time of any valid sequential test under the alternative&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17587v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:11:02 +0000</pubDate>
    </item>
    <item>
      <title>AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games</title>
      <link>http://arxiv.org/abs/2602.17594v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lance Ying, Ryan Truong, Prafull Sharma, Kaiya Ivy Zhao, Nathan Cloos, Kelsey R. Allen, Thomas L. Griffiths, Katherine M. Collins&lt;/p&gt;&lt;p&gt;Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17594v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:17:25 +0000</pubDate>
    </item>
    <item>
      <title>MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models</title>
      <link>http://arxiv.org/abs/2602.17602v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hojung Jung, Rodrigo Hormazabal, Jaehyeong Jo, Youngrok Park, Kyunggeun Roh, Se-Young Yun, Sehui Han, Dae-Woong Jeong&lt;/p&gt;&lt;p&gt;Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17602v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:27:11 +0000</pubDate>
    </item>
    <item>
      <title>SOLVAR: Fast covariance-based heterogeneity analysis with pose refinement for cryo-EM</title>
      <link>http://arxiv.org/abs/2602.17603v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Roey Yadgar, Roy R. Lederman, Yoel Shkolnisky&lt;/p&gt;&lt;p&gt;Cryo-electron microscopy (cryo-EM) has emerged as a powerful technique for resolving the three-dimensional structures of macromolecules. A key challenge in cryo-EM is characterizing continuous heterogeneity, where molecules adopt a continuum of conformational states. Covariance-based methods offer a principled approach to modeling structural variability&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17603v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:28:46 +0000</pubDate>
    </item>
    <item>
      <title>Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</title>
      <link>http://arxiv.org/abs/2602.17605v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jowaria Khan, Anindya Sarkar, Yevgeniy Vorobeychik, Elizabeth Bondi-Kelly&lt;/p&gt;&lt;p&gt;In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17605v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:30:18 +0000</pubDate>
    </item>
    <item>
      <title>AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing</title>
      <link>http://arxiv.org/abs/2602.17607v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jianda Du, Youran Sun, Haizhao Yang&lt;/p&gt;&lt;p&gt;PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17607v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:31:52 +0000</pubDate>
    </item>
    <item>
      <title>Towards Anytime-Valid Statistical Watermarking</title>
      <link>http://arxiv.org/abs/2602.17608v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Baihe Huang, Eric Xu, Kannan Ramchandran, Jiantao Jiao, Michael I. Jordan&lt;/p&gt;&lt;p&gt;The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid in&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17608v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:32:26 +0000</pubDate>
    </item>
    <item>
      <title>Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</title>
      <link>http://arxiv.org/abs/2602.17616v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Luke Huang, Zhuoyang Zhang, Qinghao Hu, Shang Yang, Song Han&lt;/p&gt;&lt;p&gt;Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17616v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:40:51 +0000</pubDate>
    </item>
    <item>
      <title>When to Trust the Cheap Check: Weak and Strong Verification for Reasoning</title>
      <link>http://arxiv.org/abs/2602.17633v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shayan Kiyani, Sima Noorani, George Pappas, Hamed Hassani&lt;/p&gt;&lt;p&gt;Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17633v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:47:38 +0000</pubDate>
    </item>
    <item>
      <title>Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting</title>
      <link>http://arxiv.org/abs/2602.17634v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim&lt;/p&gt;&lt;p&gt;Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17634v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:48:08 +0000</pubDate>
    </item>
    <item>
      <title>FAMOSE: A ReAct Approach to Automated Feature Discovery</title>
      <link>http://arxiv.org/abs/2602.17641v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Keith Burghardt, Jienan Liu, Sadman Sakib, Yuning Hao, Bo Li&lt;/p&gt;&lt;p&gt;Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17641v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:53:15 +0000</pubDate>
    </item>
    <item>
      <title>Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</title>
      <link>http://arxiv.org/abs/2602.17645v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaohan Zhao, Zhaoyi Li, Yaxin Luo, Jiacheng Cui, Zhiqiang Shen&lt;/p&gt;&lt;p&gt;Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17645v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:54:32 +0000</pubDate>
    </item>
    <item>
      <title>MARS: Margin-Aware Reward-Modeling with Self-Refinement</title>
      <link>http://arxiv.org/abs/2602.17658v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Payel Bhattacharjee, Osvaldo Simeone, Ravi Tandon&lt;/p&gt;&lt;p&gt;Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17658v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:59:03 +0000</pubDate>
    </item>
    <item>
      <title>CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</title>
      <link>http://arxiv.org/abs/2602.17663v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Juri Opitz, Corina Raclé, Emanuela Boros, Andrianos Michail, Matteo Romanello, Maud Ehrmann, Simon Clematide&lt;/p&gt;&lt;p&gt;HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17663v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:59:44 +0000</pubDate>
    </item>
    <item>
      <title>Sink-Aware Pruning for Diffusion Language Models</title>
      <link>http://arxiv.org/abs/2602.17664v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen&lt;/p&gt;&lt;p&gt;Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR model&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.17664v1</guid>
      <pubDate>Thu, 19 Feb 2026 18:59:50 +0000</pubDate>
    </item>
  </channel>
</rss>
