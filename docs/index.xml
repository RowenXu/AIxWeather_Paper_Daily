<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 03 Feb 2026 04:16:15 +0000</lastBuildDate>
    <item>
      <title>Structure Enables Effective Self-Localization of Errors in LLMs</title>
      <link>http://arxiv.org/abs/2602.02416v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ankur Samanta, Akshayaa Magesh, Ayush Jain, Kavosh Asadi, Youliang Yu, Daniel Jiang, Boris Vidolov, Kaveh Hassani&lt;/p&gt;&lt;p&gt;Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02416v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:15:59 +0000</pubDate>
    </item>
    <item>
      <title>SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration</title>
      <link>http://arxiv.org/abs/2602.02419v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qingni Wang, Yue Fan, Xin Eric Wang&lt;/p&gt;&lt;p&gt;Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02419v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:22:45 +0000</pubDate>
    </item>
    <item>
      <title>Poly-attention: a general scheme for higher-order self-attention</title>
      <link>http://arxiv.org/abs/2602.02422v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sayak Chakrabarti, Toniann Pitassi, Josh Alman&lt;/p&gt;&lt;p&gt;The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic ru&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02422v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:24:53 +0000</pubDate>
    </item>
    <item>
      <title>Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning</title>
      <link>http://arxiv.org/abs/2602.02431v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Filip Kovačević, Hong Chang Ji, Denny Wu, Mahdi Soltanolkotabi, Marco Mondelli&lt;/p&gt;&lt;p&gt;It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02431v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:31:51 +0000</pubDate>
    </item>
    <item>
      <title>UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</title>
      <link>http://arxiv.org/abs/2602.02437v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang&lt;/p&gt;&lt;p&gt;Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02437v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:34:35 +0000</pubDate>
    </item>
    <item>
      <title>Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2602.02451v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Patrick Cooper, Alvaro Velasquez&lt;/p&gt;&lt;p&gt;Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02451v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:43:52 +0000</pubDate>
    </item>
    <item>
      <title>Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling</title>
      <link>http://arxiv.org/abs/2602.02453v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang, Tiejun Zhao&lt;/p&gt;&lt;p&gt;Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02453v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:43:57 +0000</pubDate>
    </item>
    <item>
      <title>World-Gymnast: Training Robots with Reinforcement Learning in a World Model</title>
      <link>http://arxiv.org/abs/2602.02454v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, Sherry Yang&lt;/p&gt;&lt;p&gt;Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better rea&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02454v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:44:45 +0000</pubDate>
    </item>
    <item>
      <title>Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</title>
      <link>http://arxiv.org/abs/2602.02455v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Han Bao, Zheyuan Zhang, Pengcheng Jing, Zhengqing Yuan, Kaiwen Shi, Yanfang Ye&lt;/p&gt;&lt;p&gt;As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input fau&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02455v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:46:16 +0000</pubDate>
    </item>
    <item>
      <title>Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2602.02462v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Gabriele Maraia, Marco Valentino, Fabio Massimo Zanzotto, Leonardo Ranaldi&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; how&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02462v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:48:44 +0000</pubDate>
    </item>
    <item>
      <title>MentisOculi: Revealing the Limits of Reasoning with Mental Imagery</title>
      <link>http://arxiv.org/abs/2602.02465v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Prasanna Mayilvahanan, Matthias Bethge, Felix Wichmann, Ryan Cotterell&lt;/p&gt;&lt;p&gt;Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02465v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:49:06 +0000</pubDate>
    </item>
    <item>
      <title>Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts</title>
      <link>http://arxiv.org/abs/2602.02468v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang&lt;/p&gt;&lt;p&gt;Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02468v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:50:07 +0000</pubDate>
    </item>
    <item>
      <title>Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</title>
      <link>http://arxiv.org/abs/2602.02470v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi&lt;/p&gt;&lt;p&gt;Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather t&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02470v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:50:57 +0000</pubDate>
    </item>
    <item>
      <title>Multi-head automated segmentation by incorporating detection head into the contextual layer neural network</title>
      <link>http://arxiv.org/abs/2602.02471v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Edwin Kys, Febian Febian&lt;/p&gt;&lt;p&gt;Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in ana&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02471v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:51:25 +0000</pubDate>
    </item>
    <item>
      <title>MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents</title>
      <link>http://arxiv.org/abs/2602.02474v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haozhen Zhang, Quanyu Long, Jianzhu Bao, Tao Feng, Weizhi Zhang, Haodong Yue, Wenya Wang&lt;/p&gt;&lt;p&gt;Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02474v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:53:28 +0000</pubDate>
    </item>
    <item>
      <title>AgentRx: Diagnosing AI Agent Failures from Execution Trajectories</title>
      <link>http://arxiv.org/abs/2602.02475v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shraddha Barke, Arnav Goyal, Alind Khare, Avaljot Singh, Suman Nath, Chetan Bansal&lt;/p&gt;&lt;p&gt;AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02475v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:54:07 +0000</pubDate>
    </item>
    <item>
      <title>RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents</title>
      <link>http://arxiv.org/abs/2602.02486v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu&lt;/p&gt;&lt;p&gt;LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02486v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:58:07 +0000</pubDate>
    </item>
    <item>
      <title>New explanations and inference for least angle regression</title>
      <link>http://arxiv.org/abs/2602.02491v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Karl B. Gregory, Daniel J. Nordman&lt;/p&gt;&lt;p&gt;Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a "black box," where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02491v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:59:39 +0000</pubDate>
    </item>
    <item>
      <title>PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</title>
      <link>http://arxiv.org/abs/2602.02493v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zehong Ma, Ruihan Xu, Shiliang Zhang&lt;/p&gt;&lt;p&gt;Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02493v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:59:42 +0000</pubDate>
    </item>
    <item>
      <title>Reward-free Alignment for Conflicting Objectives</title>
      <link>http://arxiv.org/abs/2602.02495v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Peter Chen, Xiaopeng Li, Xi Chen, Tianyi Lin&lt;/p&gt;&lt;p&gt;Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.02495v1</guid>
      <pubDate>Mon, 02 Feb 2026 18:59:52 +0000</pubDate>
    </item>
  </channel>
</rss>
