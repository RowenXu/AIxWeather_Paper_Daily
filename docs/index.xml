<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 04 Feb 2026 04:11:44 +0000</lastBuildDate>
    <item>
      <title>Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives</title>
      <link>http://arxiv.org/abs/2602.03750v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Owen Dong, Lily Gao, Manish Kota, Bennett A. Landmana, Jelena Bekvalac, Gaynor Western, Katherine D. Van Schaik&lt;/p&gt;&lt;p&gt;Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03750v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:14:23 +0000</pubDate>
    </item>
    <item>
      <title>Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon</title>
      <link>http://arxiv.org/abs/2602.03767v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Rajat Masiwal, Colin Aitken, Adam Marchakitus, Mayank Gupta, Katherine Kowal, Hamid A. Pahlavan, Tyler Yang, Y. Qiang Sun&lt;/p&gt;&lt;p&gt;Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03767v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:27:22 +0000</pubDate>
    </item>
    <item>
      <title>UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining</title>
      <link>http://arxiv.org/abs/2602.03772v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Changhao Wang, Yunfei Yu, Xinhao Yao, Jiaolong Yang, Riccardo Cantoro, Chaobo Li, Qing Cui, Jun Zhou&lt;/p&gt;&lt;p&gt;The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \textit{manifold approximation} problem without training proxy models or relying on external reference datasets&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03772v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:32:56 +0000</pubDate>
    </item>
    <item>
      <title>An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents</title>
      <link>http://arxiv.org/abs/2602.03775v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Farnoosh Hashemi, Michael W. Macy&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03775v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:34:32 +0000</pubDate>
    </item>
    <item>
      <title>DiffLOB: Diffusion Models for Counterfactual Generation in Limit Order Books</title>
      <link>http://arxiv.org/abs/2602.03776v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhuohan Wang, Carmine Ventre&lt;/p&gt;&lt;p&gt;Modern generative models for limit order books (LOBs) can reproduce realistic market dynamics, but remain fundamentally passive: they either model what typically happens without accounting for hypothetical future market conditions, or they require interaction with another agent to explore alternative outcomes. This limits their usefulness for stress testing, scenario analysis, and decision-making. We propose \textbf{DiffLOB}, a regime-conditioned \textbf{Diff}usion model for controllable and counterfactual generation of \textbf{LOB} trajectories&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03776v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:34:56 +0000</pubDate>
    </item>
    <item>
      <title>Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity</title>
      <link>http://arxiv.org/abs/2602.03778v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Aneri Muni, Vincent Taboga, Esther Derman, Pierre-Luc Bacon, Erick Delage&lt;/p&gt;&lt;p&gt;Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03778v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:39:45 +0000</pubDate>
    </item>
    <item>
      <title>Efficient Estimation of Kernel Surrogate Models for Task Attribution</title>
      <link>http://arxiv.org/abs/2602.03783v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang&lt;/p&gt;&lt;p&gt;Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03783v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:43:48 +0000</pubDate>
    </item>
    <item>
      <title>AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration</title>
      <link>http://arxiv.org/abs/2602.03786v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jianhao Ruan, Zhihao Xu, Yiran Peng, Fashen Ren, Zhaoyang Yu, Xinbing Liang, Jinyu Xiang, Bang Liu&lt;/p&gt;&lt;p&gt;Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03786v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:46:16 +0000</pubDate>
    </item>
    <item>
      <title>Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants</title>
      <link>http://arxiv.org/abs/2602.03789v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Gabriel Damsholt, Jes Frellsen, Susanne Ditlevsen&lt;/p&gt;&lt;p&gt;Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03789v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:48:34 +0000</pubDate>
    </item>
    <item>
      <title>Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity</title>
      <link>http://arxiv.org/abs/2602.03794v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yingxuan Yang, Chengrui Qu, Muning Wen, Laixi Shi, Ying Wen, Weinan Zhang, Adam Wierman, Shangding Gu&lt;/p&gt;&lt;p&gt;LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03794v1</guid>
      <pubDate>Tue, 03 Feb 2026 17:58:10 +0000</pubDate>
    </item>
    <item>
      <title>Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</title>
      <link>http://arxiv.org/abs/2602.03806v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun&lt;/p&gt;&lt;p&gt;Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03806v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:08:41 +0000</pubDate>
    </item>
    <item>
      <title>Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network</title>
      <link>http://arxiv.org/abs/2602.03808v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Abdul Joseph Fofanah, Lian Wen, David Chen, Shaoyang Zhang&lt;/p&gt;&lt;p&gt;Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) clas&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03808v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:10:40 +0000</pubDate>
    </item>
    <item>
      <title>Antidistillation Fingerprinting</title>
      <link>http://arxiv.org/abs/2602.03812v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yixuan Even Xu, John Kirchenbauer, Yash Savani, Asher Trockman, Alexander Robey, Tom Goldstein, Fei Fang, J. Zico Kolter&lt;/p&gt;&lt;p&gt;Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a pri&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03812v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:15:50 +0000</pubDate>
    </item>
    <item>
      <title>Conformal Thinking: Risk Control for Reasoning on a Compute Budget</title>
      <link>http://arxiv.org/abs/2602.03814v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xi Wang, Anushri Suresh, Alvin Zhang, Rishi More, William Jurayj, Benjamin Van Durme, Mehrdad Farajtabar, Daniel Khashabi&lt;/p&gt;&lt;p&gt;Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03814v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:17:22 +0000</pubDate>
    </item>
    <item>
      <title>Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion</title>
      <link>http://arxiv.org/abs/2602.03817v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Oscar Ovanger, Levi Harris, Timothy H. Keitt&lt;/p&gt;&lt;p&gt;Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \textbf{F}usion under \textbf{IN}dependent \textbf{C}onditio&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03817v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:21:13 +0000</pubDate>
    </item>
    <item>
      <title>Preference-based Conditional Treatment Effects and Policy Learning</title>
      <link>http://arxiv.org/abs/2602.03823v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dovid Parnas, Mathieu Even, Julie Josse, Uri Shalit&lt;/p&gt;&lt;p&gt;We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditional probability of necessity and sufficiency, conditional Win Ratio, and Generalized Pairwise Comparisons&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03823v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:31:26 +0000</pubDate>
    </item>
    <item>
      <title>AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations</title>
      <link>http://arxiv.org/abs/2602.03828v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Minjun Zhu, Zhen Lin, Yixuan Weng, Panzhong Lu, Qiujie Xie, Yifan Wei, Sifan Liu, Qiyao Sun&lt;/p&gt;&lt;p&gt;High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03828v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:41:43 +0000</pubDate>
    </item>
    <item>
      <title>Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</title>
      <link>http://arxiv.org/abs/2602.03837v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner&lt;/p&gt;&lt;p&gt;Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theo&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03837v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:56:17 +0000</pubDate>
    </item>
    <item>
      <title>PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization</title>
      <link>http://arxiv.org/abs/2602.03838v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Erzhen Hu, Frederik Brudy, David Ledo, George Fitzmaurice, Fraser Anderson&lt;/p&gt;&lt;p&gt;In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03838v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:56:40 +0000</pubDate>
    </item>
    <item>
      <title>PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning</title>
      <link>http://arxiv.org/abs/2602.03846v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Romain Cosentino&lt;/p&gt;&lt;p&gt;We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.03846v1</guid>
      <pubDate>Tue, 03 Feb 2026 18:59:42 +0000</pubDate>
    </item>
  </channel>
</rss>
