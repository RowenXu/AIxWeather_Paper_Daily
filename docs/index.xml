<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 26 Feb 2026 04:18:37 +0000</lastBuildDate>
    <item>
      <title>RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models</title>
      <link>http://arxiv.org/abs/2602.22026v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaoyu Xian, Shiao Wang, Xiao Wang, Daxin Tian, Yan Tian&lt;/p&gt;&lt;p&gt;Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle these difficulties, we explore the integration of event cameras into the perception system, leveraging their advantages in low-light conditions, high-speed scenarios, and low power consumption&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22026v1</guid>
      <pubDate>Wed, 25 Feb 2026 15:34:15 +0000</pubDate>
    </item>
    <item>
      <title>TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition</title>
      <link>http://arxiv.org/abs/2602.22039v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Cheng-Yeh Yang, Chien-Chun Wang, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen&lt;/p&gt;&lt;p&gt;Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recognition, a translation-guided ASR framework that utilizes multilingual translation embeddings to enhan&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22039v1</guid>
      <pubDate>Wed, 25 Feb 2026 15:47:34 +0000</pubDate>
    </item>
    <item>
      <title>Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach</title>
      <link>http://arxiv.org/abs/2602.22055v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hamza Haruna Mohammed, Dusica Marijan, Arnbjørn Maressa&lt;/p&gt;&lt;p&gt;Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-driven approaches achieve accuracy at the expense of physical plausibility. This paper introduces a Physics-Informed Kolmogorov-Arnold Network (PI-KAN), a hybrid method that integrates interpretable univariate feature transformations with a physics-informed loss function and a leakage-free chained predic&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22055v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:06:28 +0000</pubDate>
    </item>
    <item>
      <title>NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training</title>
      <link>http://arxiv.org/abs/2602.22059v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dengdi Sun, Xiaoya Zhou, Xiao Wang, Hao Si, Wanli Lyu, Jin Tang, Bin Luo&lt;/p&gt;&lt;p&gt;Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22059v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:08:46 +0000</pubDate>
    </item>
    <item>
      <title>DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2602.22066v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jinpeng Li, Zhongyi Pei, Huaze Xue, Bojian Zheng, Chen Wang, Jianmin Wang&lt;/p&gt;&lt;p&gt;Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFMs) for multivariate forecasting by using a pair of learnable, structurally symmetric surrogate series. Generated by a shared auxiliary feature-fusion module that captures cross-variable dependencies, these surrogates are mapped to TSFM-compatible series via the forecasting objective&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22066v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:13:12 +0000</pubDate>
    </item>
    <item>
      <title>Semantic Partial Grounding via LLMs</title>
      <link>http://arxiv.org/abs/2602.22067v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Giuseppe Canonaco, Alberto Pozanco, Daniel Borrajo&lt;/p&gt;&lt;p&gt;Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22067v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:13:26 +0000</pubDate>
    </item>
    <item>
      <title>Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts</title>
      <link>http://arxiv.org/abs/2602.22070v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jessica Y. Bo, Lillio Mok, Ashton Anderson&lt;/p&gt;&lt;p&gt;Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algor&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22070v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:18:28 +0000</pubDate>
    </item>
    <item>
      <title>Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2602.22072v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Christian Nickel, Laura Schrewe, Florian Mai, Lucie Flek&lt;/p&gt;&lt;p&gt;Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoni&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22072v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:24:35 +0000</pubDate>
    </item>
    <item>
      <title>Coarsening Bias from Variable Discretization in Causal Functionals</title>
      <link>http://arxiv.org/abs/2602.22083v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaxian Ou, Razieh Nabi&lt;/p&gt;&lt;p&gt;A class of causal effect functionals requires integration over conditional densities of continuous variables, as in mediation effects and nonparametric identification in causal graphical models. Estimating such densities and evaluating the resulting integrals can be statistically and computationally demanding. A common workaround is to discretize the variable and replace integrals with finite sums&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22083v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:32:04 +0000</pubDate>
    </item>
    <item>
      <title>On Imbalanced Regression with Hoeffding Trees</title>
      <link>http://arxiv.org/abs/2602.22101v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Pantia-Marina Alchirch, Dimitrios I. Diochnos&lt;/p&gt;&lt;p&gt;Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembles. At the same time a recent line of work in batch learning has shown that kernel density estimation (KDE) is an effective approach for smoothed predictions in imbalanced regression tasks [Yang et al., 2021]&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22101v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:48:07 +0000</pubDate>
    </item>
    <item>
      <title>Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection</title>
      <link>http://arxiv.org/abs/2602.22107v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Andrea Apicella, Francesco Isgrò, Andrea Pollastro, Roberto Prevete&lt;/p&gt;&lt;p&gt;Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22107v1</guid>
      <pubDate>Wed, 25 Feb 2026 16:56:14 +0000</pubDate>
    </item>
    <item>
      <title>Probing the Geometry of Diffusion Models with the String Method</title>
      <link>http://arxiv.org/abs/2602.22122v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Elio Moreau, Florentin Coeurdoux, Grégoire Ferre, Eric Vanden-Eijnden&lt;/p&gt;&lt;p&gt;Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. We introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22122v1</guid>
      <pubDate>Wed, 25 Feb 2026 17:10:59 +0000</pubDate>
    </item>
    <item>
      <title>SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents</title>
      <link>http://arxiv.org/abs/2602.22124v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Patrick Tser Jern Kon, Archana Pradeep, Ang Chen, Alexander P. Ellis, Warren Hunt, Zijian Wang, John Yang, Samuel Thompson&lt;/p&gt;&lt;p&gt;Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22124v1</guid>
      <pubDate>Wed, 25 Feb 2026 17:11:49 +0000</pubDate>
    </item>
    <item>
      <title>NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors</title>
      <link>http://arxiv.org/abs/2602.22144v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lingfeng Ren, Weihao Yu, Runpeng Yu, Xinchao Wang&lt;/p&gt;&lt;p&gt;Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal th&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22144v1</guid>
      <pubDate>Wed, 25 Feb 2026 17:50:41 +0000</pubDate>
    </item>
    <item>
      <title>When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models</title>
      <link>http://arxiv.org/abs/2602.22145v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Satyam Kumar Navneet, Joydeep Chandra, Yong Zhang&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce "Cultural Ghosting", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,&amp; Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) &amp; Semantic Preservation Score (SPS)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22145v1</guid>
      <pubDate>Wed, 25 Feb 2026 17:54:42 +0000</pubDate>
    </item>
    <item>
      <title>Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual</title>
      <link>http://arxiv.org/abs/2602.22146v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yining Li, Peizhong Ju, Ness Shroff&lt;/p&gt;&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22146v1</guid>
      <pubDate>Wed, 25 Feb 2026 17:54:52 +0000</pubDate>
    </item>
    <item>
      <title>Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach</title>
      <link>http://arxiv.org/abs/2602.22188v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Nathalie C. Pinheiro, Donghu Guo, Hannah P. Menke, Aniket C. Joshi, Claire E. Heaney, Ahmed H. ElSheikh, Christopher C. Pain&lt;/p&gt;&lt;p&gt;Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22188v1</guid>
      <pubDate>Wed, 25 Feb 2026 18:34:03 +0000</pubDate>
    </item>
    <item>
      <title>GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL</title>
      <link>http://arxiv.org/abs/2602.22190v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang, Hao Cheng, Huaxiu Yao, Baoling Peng&lt;/p&gt;&lt;p&gt;Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22190v1</guid>
      <pubDate>Wed, 25 Feb 2026 18:34:57 +0000</pubDate>
    </item>
    <item>
      <title>Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</title>
      <link>http://arxiv.org/abs/2602.22197v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath&lt;/p&gt;&lt;p&gt;Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22197v1</guid>
      <pubDate>Wed, 25 Feb 2026 18:46:30 +0000</pubDate>
    </item>
    <item>
      <title>Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets</title>
      <link>http://arxiv.org/abs/2602.22207v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hanna Yukhymenko, Anton Alexandrov, Martin Vechev&lt;/p&gt;&lt;p&gt;The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.22207v1</guid>
      <pubDate>Wed, 25 Feb 2026 18:58:25 +0000</pubDate>
    </item>
  </channel>
</rss>
