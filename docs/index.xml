<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 31 Dec 2025 03:27:57 +0000</lastBuildDate>
    <item>
      <title>Act2Goal: From World Model To General Goal-conditioned Policy</title>
      <link>http://arxiv.org/abs/2512.23541v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo&lt;/p&gt;&lt;p&gt;Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23541v1</guid>
      <pubDate>Mon, 29 Dec 2025 15:28:42 +0000</pubDate>
    </item>
    <item>
      <title>PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</title>
      <link>http://arxiv.org/abs/2512.23545v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li&lt;/p&gt;&lt;p&gt;Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23545v1</guid>
      <pubDate>Mon, 29 Dec 2025 15:34:27 +0000</pubDate>
    </item>
    <item>
      <title>Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs</title>
      <link>http://arxiv.org/abs/2512.23547v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sahil Kale, Antonio Luca Alfeo&lt;/p&gt;&lt;p&gt;Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23547v1</guid>
      <pubDate>Mon, 29 Dec 2025 15:41:13 +0000</pubDate>
    </item>
    <item>
      <title>Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks</title>
      <link>http://arxiv.org/abs/2512.23557v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty&lt;/p&gt;&lt;p&gt;Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In ord&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23557v1</guid>
      <pubDate>Mon, 29 Dec 2025 15:54:33 +0000</pubDate>
    </item>
    <item>
      <title>VL-RouterBench: A Benchmark for Vision-Language Model Routing</title>
      <link>http://arxiv.org/abs/2512.23562v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang&lt;/p&gt;&lt;p&gt;Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23562v1</guid>
      <pubDate>Mon, 29 Dec 2025 16:01:19 +0000</pubDate>
    </item>
    <item>
      <title>RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</title>
      <link>http://arxiv.org/abs/2512.23565v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao&lt;/p&gt;&lt;p&gt;The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Doc&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23565v1</guid>
      <pubDate>Mon, 29 Dec 2025 16:05:38 +0000</pubDate>
    </item>
    <item>
      <title>From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints</title>
      <link>http://arxiv.org/abs/2512.23566v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dimitra Maoutsa&lt;/p&gt;&lt;p&gt;How can we learn the laws underlying the dynamics of stochastic systems when their trajectories are sampled sparsely in time? Existing methods either require temporally resolved high-frequency observations, or rely on geometric arguments that apply only to conservative systems, limiting the range of dynamics they can recover. Here, we present a new framework that reconciles these two perspectives by reformulating inference as a stochastic control problem. Our method uses geometry-driven path augmentation, guided by the geometry in the system's invariant density to reconstruct likely trajectori&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23566v1</guid>
      <pubDate>Mon, 29 Dec 2025 16:06:08 +0000</pubDate>
    </item>
    <item>
      <title>The Nonstationarity-Complexity Tradeoff in Return Prediction</title>
      <link>http://arxiv.org/abs/2512.23596v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou&lt;/p&gt;&lt;p&gt;We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estima&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23596v1</guid>
      <pubDate>Mon, 29 Dec 2025 16:49:19 +0000</pubDate>
    </item>
    <item>
      <title>Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation</title>
      <link>http://arxiv.org/abs/2512.23601v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Manh Hung Nguyen, Adish Singla&lt;/p&gt;&lt;p&gt;Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23601v1</guid>
      <pubDate>Mon, 29 Dec 2025 16:53:48 +0000</pubDate>
    </item>
    <item>
      <title>Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE</title>
      <link>http://arxiv.org/abs/2512.23624v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chien-Ting Tung, Chenming Hu&lt;/p&gt;&lt;p&gt;We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23624v1</guid>
      <pubDate>Mon, 29 Dec 2025 17:28:35 +0000</pubDate>
    </item>
    <item>
      <title>Regret-Based Federated Causal Discovery with Unknown Interventions</title>
      <link>http://arxiv.org/abs/2512.23626v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Federico Baldo, Charles K. Assaad&lt;/p&gt;&lt;p&gt;Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23626v1</guid>
      <pubDate>Mon, 29 Dec 2025 17:30:01 +0000</pubDate>
    </item>
    <item>
      <title>BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization</title>
      <link>http://arxiv.org/abs/2512.23631v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong&lt;/p&gt;&lt;p&gt;Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23631v1</guid>
      <pubDate>Mon, 29 Dec 2025 17:41:11 +0000</pubDate>
    </item>
    <item>
      <title>AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms</title>
      <link>http://arxiv.org/abs/2512.23633v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; LearnLM Team, Eedi, :, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty&lt;/p&gt;&lt;p&gt;One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23633v1</guid>
      <pubDate>Mon, 29 Dec 2025 17:44:03 +0000</pubDate>
    </item>
    <item>
      <title>Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks</title>
      <link>http://arxiv.org/abs/2512.23643v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Konstantin Yakovlev, Nikita Puchkin&lt;/p&gt;&lt;p&gt;We present a theory for simultaneous approximation of the score function and its derivatives, enabling the handling of data distributions with low-dimensional structure and unbounded support. Our approximation error bounds match those in the literature while relying on assumptions that relax the usual bounded support requirement. Crucially, our bounds are free from the curse of dimensionality&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23643v1</guid>
      <pubDate>Mon, 29 Dec 2025 17:54:45 +0000</pubDate>
    </item>
    <item>
      <title>Random Controlled Differential Equations</title>
      <link>http://arxiv.org/abs/2512.23670v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Francesco Piatti, Thomas Cass, William F. Turner&lt;/p&gt;&lt;p&gt;We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23670v1</guid>
      <pubDate>Mon, 29 Dec 2025 18:25:10 +0000</pubDate>
    </item>
    <item>
      <title>Web World Models</title>
      <link>http://arxiv.org/abs/2512.23676v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang&lt;/p&gt;&lt;p&gt;Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level de&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23676v1</guid>
      <pubDate>Mon, 29 Dec 2025 18:31:45 +0000</pubDate>
    </item>
    <item>
      <title>Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing</title>
      <link>http://arxiv.org/abs/2512.23684v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai. -Doss&lt;/p&gt;&lt;p&gt;Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23684v1</guid>
      <pubDate>Mon, 29 Dec 2025 18:43:05 +0000</pubDate>
    </item>
    <item>
      <title>Bellman Calibration for V-Learning in Offline Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2512.23694v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lars van der Laan, Nathan Kallus&lt;/p&gt;&lt;p&gt;We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.23694v1</guid>
      <pubDate>Mon, 29 Dec 2025 18:52:18 +0000</pubDate>
    </item>
  </channel>
</rss>
