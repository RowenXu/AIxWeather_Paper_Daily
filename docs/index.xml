<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 07 Feb 2026 04:09:20 +0000</lastBuildDate>
    <item>
      <title>Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025</title>
      <link>http://arxiv.org/abs/2602.05930v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Samar Ansari&lt;/p&gt;&lt;p&gt;Large language models (LLMs) are increasingly used in academic writing workflows, yet they frequently hallucinate by generating citations to sources that do not exist. This study analyzes 100 AI-generated hallucinated citations that appeared in papers accepted by the 2025 Conference on Neural Information Processing Systems (NeurIPS), one of the world's most prestigious AI conferences. Despite review by 3-5 expert researchers per paper, these fabricated citations evaded detection, appearing in 53 published papers (approx&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05930v1</guid>
      <pubDate>Thu, 05 Feb 2026 17:43:35 +0000</pubDate>
    </item>
    <item>
      <title>Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</title>
      <link>http://arxiv.org/abs/2602.05951v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim&lt;/p&gt;&lt;p&gt;Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05951v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:08:20 +0000</pubDate>
    </item>
    <item>
      <title>Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces</title>
      <link>http://arxiv.org/abs/2602.05961v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Arran Carter, Sanghyeok Choi, Kirill Tamogashev, Víctor Elvira, Nikolay Malkin&lt;/p&gt;&lt;p&gt;Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05961v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:16:57 +0000</pubDate>
    </item>
    <item>
      <title>LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation</title>
      <link>http://arxiv.org/abs/2602.05966v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mirlan Karimov, Teodora Spasojevic, Markus Braun, Julian Wiederer, Vasileios Belagiannis, Marc Pollefeys&lt;/p&gt;&lt;p&gt;Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05966v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:21:02 +0000</pubDate>
    </item>
    <item>
      <title>Inverse Depth Scaling From Most Layers Being Similar</title>
      <link>http://arxiv.org/abs/2602.05970v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yizhou Liu, Sara Kangaslahti, Ziming Liu, Jeff Gore&lt;/p&gt;&lt;p&gt;Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05970v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:22:41 +0000</pubDate>
    </item>
    <item>
      <title>Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins</title>
      <link>http://arxiv.org/abs/2602.05983v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Krešimir Kušić, Vinny Cahill, Ivana Dusparic&lt;/p&gt;&lt;p&gt;The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05983v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:33:03 +0000</pubDate>
    </item>
    <item>
      <title>RISE-Video: Can Video Generators Decode Implicit World Rules?</title>
      <link>http://arxiv.org/abs/2602.05986v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mingxin Liu, Shuran Ma, Shibei Meng, Xiangyu Zhao, Zicheng Zhang, Shaofeng Zhang, Zhihang Zhong, Peixian Chen&lt;/p&gt;&lt;p&gt;While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05986v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:36:10 +0000</pubDate>
    </item>
    <item>
      <title>Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</title>
      <link>http://arxiv.org/abs/2602.05993v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola&lt;/p&gt;&lt;p&gt;Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05993v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:42:00 +0000</pubDate>
    </item>
    <item>
      <title>Orthogonal Self-Attention</title>
      <link>http://arxiv.org/abs/2602.05996v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Leo Zhang, James Martens&lt;/p&gt;&lt;p&gt;Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05996v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:42:57 +0000</pubDate>
    </item>
    <item>
      <title>Causal Inference on Stopped Random Walks in Online Advertising</title>
      <link>http://arxiv.org/abs/2602.05997v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jia Yuan Yu&lt;/p&gt;&lt;p&gt;We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an a&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.05997v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:43:29 +0000</pubDate>
    </item>
    <item>
      <title>Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods</title>
      <link>http://arxiv.org/abs/2602.06000v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ali Shendabadi, Parnia Izadirad, Mostafa Salehi, Mahmoud Bijankhan&lt;/p&gt;&lt;p&gt;Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06000v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:46:28 +0000</pubDate>
    </item>
    <item>
      <title>AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</title>
      <link>http://arxiv.org/abs/2602.06008v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xianyang Liu, Shangding Gu, Dawn Song&lt;/p&gt;&lt;p&gt;Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06008v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:50:36 +0000</pubDate>
    </item>
    <item>
      <title>GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</title>
      <link>http://arxiv.org/abs/2602.06013v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang&lt;/p&gt;&lt;p&gt;The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06013v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:52:48 +0000</pubDate>
    </item>
    <item>
      <title>Optimism Stabilizes Thompson Sampling for Adaptive Inference</title>
      <link>http://arxiv.org/abs/2602.06014v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shunxing Yan, Han Zhong&lt;/p&gt;&lt;p&gt;Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06014v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:52:54 +0000</pubDate>
    </item>
    <item>
      <title>Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</title>
      <link>http://arxiv.org/abs/2602.06021v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ye He, Yitong Qiu, Molei Tao&lt;/p&gt;&lt;p&gt;When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model's performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neigh&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06021v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:55:03 +0000</pubDate>
    </item>
    <item>
      <title>Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</title>
      <link>http://arxiv.org/abs/2602.06022v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Miranda Muqing Miao, Young-Min Cho, Lyle Ungar&lt;/p&gt;&lt;p&gt;Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06022v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:55:56 +0000</pubDate>
    </item>
    <item>
      <title>Learning Event-Based Shooter Models from Virtual Reality Experiments</title>
      <link>http://arxiv.org/abs/2602.06023v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Christopher A. McClurg, Alan R. Wagner&lt;/p&gt;&lt;p&gt;Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06023v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:56:49 +0000</pubDate>
    </item>
    <item>
      <title>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</title>
      <link>http://arxiv.org/abs/2602.06025v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li&lt;/p&gt;&lt;p&gt;Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06025v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:57:09 +0000</pubDate>
    </item>
    <item>
      <title>DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</title>
      <link>http://arxiv.org/abs/2602.06039v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao&lt;/p&gt;&lt;p&gt;Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing pri&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06039v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:59:51 +0000</pubDate>
    </item>
    <item>
      <title>Shared LoRA Subspaces for almost Strict Continual Learning</title>
      <link>http://arxiv.org/abs/2602.06043v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille&lt;/p&gt;&lt;p&gt;Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamle&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.06043v1</guid>
      <pubDate>Thu, 05 Feb 2026 18:59:58 +0000</pubDate>
    </item>
  </channel>
</rss>
