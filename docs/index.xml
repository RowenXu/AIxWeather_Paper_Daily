<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 12 Dec 2025 03:24:44 +0000</lastBuildDate>
    <item>
      <title>MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence</title>
      <link>http://arxiv.org/abs/2512.10863v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jingli Lin, Runsen Xu, Shaohao Zhu, Sihan Yang, Peizhou Cao, Yunlong Ran, Miao Hu, Chenming Zhu&lt;/p&gt;&lt;p&gt;Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10863v1</guid>
      <pubDate>Thu, 11 Dec 2025 17:57:24 +0000</pubDate>
    </item>
    <item>
      <title>UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting</title>
      <link>http://arxiv.org/abs/2512.10866v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ruslan Gokhman&lt;/p&gt;&lt;p&gt;We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10866v1</guid>
      <pubDate>Thu, 11 Dec 2025 17:59:44 +0000</pubDate>
    </item>
    <item>
      <title>Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling</title>
      <link>http://arxiv.org/abs/2512.10873v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qitian Lu, Himanshu Sharma, Michael D. Shields, Lukáš Novák&lt;/p&gt;&lt;p&gt;Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentat&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10873v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:03:29 +0000</pubDate>
    </item>
    <item>
      <title>Impact of geometry on 1D molecular-kinetics simulations of acoustic-gravity wave propagation into the exosphere</title>
      <link>http://arxiv.org/abs/2512.10887v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jose A. Perez Chavez, Orenthal J. Tucker, Shane R. Carberry Mogan, Robert E. Johnson, Christopher Blaszczak-Boxe&lt;/p&gt;&lt;p&gt;Direct Simulation Monte Carlo (DSMC) calculations of acoustic gravity wave propagation into the exobase region of a Mars-like atmosphere reveal that radial geometry can reduce wave-driven heating compared to a Cartesian model. We examine two acoustic wave (AW) modes with periods of 11 minutes (AW1) and 5.5 minutes (AW2) propagating from 100 to 320 km altitude using a radial molecular kinetics model. The wave-driven heating was reduced by 40-56% with cycle-averaged temperature gradient $\langle dT/dr \rangle$ decreasing from 9.4 K per scale height H0 to 5.6 K/H$_0$ for AW1 and from 4.4 K/H$_0$ &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10887v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:18:19 +0000</pubDate>
    </item>
    <item>
      <title>LLMs Can Assist with Proposal Selection at Large User Facilities</title>
      <link>http://arxiv.org/abs/2512.10895v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lijie Ding, Janell Thomson, Jon Taylor, Changwoo Do&lt;/p&gt;&lt;p&gt;We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10895v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:23:56 +0000</pubDate>
    </item>
    <item>
      <title>Multi-Granular Node Pruning for Circuit Discovery</title>
      <link>http://arxiv.org/abs/2512.10903v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Muhammad Umair Haider, Hammad Rizwan, Hassan Sajjad, A. B. Siddique&lt;/p&gt;&lt;p&gt;Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10903v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:32:15 +0000</pubDate>
    </item>
    <item>
      <title>SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</title>
      <link>http://arxiv.org/abs/2512.10922v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Max Zimmer, Christophe Roux, Moritz Wagner, Deborah Hendrych, Sebastian Pokutta&lt;/p&gt;&lt;p&gt;The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10922v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:47:48 +0000</pubDate>
    </item>
    <item>
      <title>Decoupled Q-Chunking</title>
      <link>http://arxiv.org/abs/2512.10926v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qiyang Li, Seohong Park, Sergey Levine&lt;/p&gt;&lt;p&gt;Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, whic&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10926v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:52:51 +0000</pubDate>
    </item>
    <item>
      <title>BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2512.10932v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shengao Wang, Wenqi Wang, Zecheng Wang, Max Whitton, Michael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu&lt;/p&gt;&lt;p&gt;Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversati&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10932v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:57:05 +0000</pubDate>
    </item>
    <item>
      <title>Any4D: Unified Feed-Forward Metric 4D Reconstruction</title>
      <link>http://arxiv.org/abs/2512.10935v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jay Karhade, Nikhil Keetha, Yuchen Zhang, Tanisha Gupta, Akash Sharma, Sebastian Scherer, Deva Ramanan&lt;/p&gt;&lt;p&gt;We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10935v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:57:39 +0000</pubDate>
    </item>
    <item>
      <title>Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</title>
      <link>http://arxiv.org/abs/2512.10936v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kristina Korotkova, Aleksandr Katrutsa&lt;/p&gt;&lt;p&gt;The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10936v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:58:17 +0000</pubDate>
    </item>
    <item>
      <title>On Decision-Making Agents and Higher-Order Causal Processes</title>
      <link>http://arxiv.org/abs/2512.10937v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Matt Wilson&lt;/p&gt;&lt;p&gt;We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10937v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:58:33 +0000</pubDate>
    </item>
    <item>
      <title>Stronger Normalization-Free Transformers</title>
      <link>http://arxiv.org/abs/2512.10938v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mingzhi Chen, Taiming Lu, Jiachen Zhu, Mingjie Sun, Zhuang Liu&lt;/p&gt;&lt;p&gt;Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10938v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:58:49 +0000</pubDate>
    </item>
    <item>
      <title>OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis</title>
      <link>http://arxiv.org/abs/2512.10940v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiang Fan, Sharath Girish, Vivek Ramanujan, Chaoyang Wang, Ashkan Mirzaei, Petr Sushko, Aliaksandr Siarohin, Sergey Tulyakov&lt;/p&gt;&lt;p&gt;Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10940v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:05 +0000</pubDate>
    </item>
    <item>
      <title>Mull-Tokens: Modality-Agnostic Latent Thinking</title>
      <link>http://arxiv.org/abs/2512.10941v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Arijit Ray, Ahmed Abdelkader, Chengzhi Mao, Bryan A. Plummer, Kate Saenko, Ranjay Krishna, Leonidas Guibas, Wen-Sheng Chu&lt;/p&gt;&lt;p&gt;Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10941v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:08 +0000</pubDate>
    </item>
    <item>
      <title>AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation</title>
      <link>http://arxiv.org/abs/2512.10943v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sharath Girish, Viacheslav Ivanov, Tsai-Shien Chen, Hao Chen, Aliaksandr Siarohin, Sergey Tulyakov&lt;/p&gt;&lt;p&gt;Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10943v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:34 +0000</pubDate>
    </item>
    <item>
      <title>ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning</title>
      <link>http://arxiv.org/abs/2512.10946v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Wendi Chen, Han Xue, Yi Wang, Fangyuan Zhou, Jun Lv, Yang Jin, Shirun Tang, Chuan Wen&lt;/p&gt;&lt;p&gt;Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10946v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:46 +0000</pubDate>
    </item>
    <item>
      <title>Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</title>
      <link>http://arxiv.org/abs/2512.10949v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yiwen Tang, Zoey Guo, Kaixin Zhu, Ray Zhang, Qizhi Chen, Dongzhi Jiang, Junli Liu, Bohan Zeng&lt;/p&gt;&lt;p&gt;Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10949v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:52 +0000</pubDate>
    </item>
    <item>
      <title>Hierarchical Dataset Selection for High-Quality Data Sharing</title>
      <link>http://arxiv.org/abs/2512.10952v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaona Zhou, Yingyan Zeng, Ran Jin, Ismini Lourentzou&lt;/p&gt;&lt;p&gt;The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their so&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10952v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:55 +0000</pubDate>
    </item>
    <item>
      <title>SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</title>
      <link>http://arxiv.org/abs/2512.10957v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yukai Shi, Weiyu Li, Zihao Wang, Hongyang Li, Xingyu Chen, Ping Tan, Lei Zhang&lt;/p&gt;&lt;p&gt;We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.10957v1</guid>
      <pubDate>Thu, 11 Dec 2025 18:59:56 +0000</pubDate>
    </item>
  </channel>
</rss>
