<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 15 Jan 2026 03:42:21 +0000</lastBuildDate>
    <item>
      <title>Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling</title>
      <link>http://arxiv.org/abs/2601.09566v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shuyang Xiang, Hao Guan&lt;/p&gt;&lt;p&gt;Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09566v1</guid>
      <pubDate>Wed, 14 Jan 2026 15:34:37 +0000</pubDate>
    </item>
    <item>
      <title>Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels</title>
      <link>http://arxiv.org/abs/2601.09579v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Fiona Murphy, Alessio Benavoli&lt;/p&gt;&lt;p&gt;Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal l&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09579v1</guid>
      <pubDate>Wed, 14 Jan 2026 15:48:53 +0000</pubDate>
    </item>
    <item>
      <title>Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms</title>
      <link>http://arxiv.org/abs/2601.09600v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Bhaskar Mitra, Nicola Neophytou, Sireesh Gururaja&lt;/p&gt;&lt;p&gt;Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of what alternative IA infrastructure we must reimagine and build to mitigate the risks of authoritarian capture of our information ecosystems&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09600v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:15:26 +0000</pubDate>
    </item>
    <item>
      <title>Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer</title>
      <link>http://arxiv.org/abs/2601.09603v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos&lt;/p&gt;&lt;p&gt;In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09603v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:23:31 +0000</pubDate>
    </item>
    <item>
      <title>Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</title>
      <link>http://arxiv.org/abs/2601.09605v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira&lt;/p&gt;&lt;p&gt;Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09605v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:25:13 +0000</pubDate>
    </item>
    <item>
      <title>DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing</title>
      <link>http://arxiv.org/abs/2601.09609v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song, Xiting Wang, Ruiming Tang, Guorui Zhou&lt;/p&gt;&lt;p&gt;Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09609v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:30:20 +0000</pubDate>
    </item>
    <item>
      <title>CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems</title>
      <link>http://arxiv.org/abs/2601.09613v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yonglin Tian, Qiyao Zhang, Wei Xu, Yutong Wang, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang&lt;/p&gt;&lt;p&gt;Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09613v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:36:26 +0000</pubDate>
    </item>
    <item>
      <title>Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust</title>
      <link>http://arxiv.org/abs/2601.09620v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Pooja Prajod, Hannes Cools, Thomas Röggla, Karthikeya Puttur Venkatraj, Amber Kusters, Alia ElKattan, Pablo Cesar, Abdallah El Ali&lt;/p&gt;&lt;p&gt;As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09620v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:45:45 +0000</pubDate>
    </item>
    <item>
      <title>Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric</title>
      <link>http://arxiv.org/abs/2601.09624v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jiali Cheng, Ziheng Chen, Chirag Agarwal, Hadi Amiri&lt;/p&gt;&lt;p&gt;Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09624v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:55:58 +0000</pubDate>
    </item>
    <item>
      <title>The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware</title>
      <link>http://arxiv.org/abs/2601.09625v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ben Nassi, Bruce Schneier, Oleg Brodt&lt;/p&gt;&lt;p&gt;The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as "prompt injection" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constit&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09625v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:57:04 +0000</pubDate>
    </item>
    <item>
      <title>From Prompt to Protocol: Fast Charging Batteries with Large Language Models</title>
      <link>http://arxiv.org/abs/2601.09626v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ge Lei, Ferran Brosa Planella, Sterling G. Baird, Samuel J. Cooper&lt;/p&gt;&lt;p&gt;Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which sim&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09626v1</guid>
      <pubDate>Wed, 14 Jan 2026 16:58:20 +0000</pubDate>
    </item>
    <item>
      <title>LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach</title>
      <link>http://arxiv.org/abs/2601.09635v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kuo Liang, Yuhang Lu, Jianming Mao, Shuyi Sun, Chunwei Yang, Congcong Zeng, Xiao Jin, Hanzhang Qin&lt;/p&gt;&lt;p&gt;Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09635v1</guid>
      <pubDate>Wed, 14 Jan 2026 17:09:57 +0000</pubDate>
    </item>
    <item>
      <title>Automating Supply Chain Disruption Monitoring via an Agentic AI Approach</title>
      <link>http://arxiv.org/abs/2601.09680v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sara AlMahri, Liming Xu, Alexandra Brintrup&lt;/p&gt;&lt;p&gt;Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09680v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:28:31 +0000</pubDate>
    </item>
    <item>
      <title>Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection</title>
      <link>http://arxiv.org/abs/2601.09684v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang&lt;/p&gt;&lt;p&gt;Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09684v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:36:22 +0000</pubDate>
    </item>
    <item>
      <title>LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models</title>
      <link>http://arxiv.org/abs/2601.09686v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ha Nguyen, Sumanta Basu&lt;/p&gt;&lt;p&gt;The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \ell_1-penalty on the off-diagonal entries&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09686v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:37:50 +0000</pubDate>
    </item>
    <item>
      <title>Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection</title>
      <link>http://arxiv.org/abs/2601.09692v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tianyi Niu, Justin Chih-Yao Chen, Genta Indra Winata, Shi-Xiong Zhang, Supriyo Chakraborty, Sambit Sahu, Yue Zhang, Elias Stengel-Eskin&lt;/p&gt;&lt;p&gt;Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09692v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:43:32 +0000</pubDate>
    </item>
    <item>
      <title>Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design</title>
      <link>http://arxiv.org/abs/2601.09693v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Lisa Schneckenreiter, Sohvi Luukkonen, Lukas Friedrich, Daniel Kuhn, Günter Klambauer&lt;/p&gt;&lt;p&gt;Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09693v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:45:08 +0000</pubDate>
    </item>
    <item>
      <title>LLMs can Compress LLMs: Adaptive Pruning by Agents</title>
      <link>http://arxiv.org/abs/2601.09694v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sai Varun Kodathala, Rakesh Vunnam&lt;/p&gt;&lt;p&gt;As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering cap&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09694v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:45:36 +0000</pubDate>
    </item>
    <item>
      <title>ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation</title>
      <link>http://arxiv.org/abs/2601.09703v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sicong Liu, Yanxian Huang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Yuchi Ma, Hongyu Zhang, Yin Zhang&lt;/p&gt;&lt;p&gt;Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09703v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:57:31 +0000</pubDate>
    </item>
    <item>
      <title>Value-Aware Numerical Representations for Transformer Language Models</title>
      <link>http://arxiv.org/abs/2601.09706v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Andreea Dutulescu, Stefan Ruseti, Mihai Dascalu&lt;/p&gt;&lt;p&gt;Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2601.09706v1</guid>
      <pubDate>Wed, 14 Jan 2026 18:59:14 +0000</pubDate>
    </item>
  </channel>
</rss>
