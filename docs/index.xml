<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 06 Dec 2025 03:10:35 +0000</lastBuildDate>
    <item>
      <title>Learning Causality for Longitudinal Data</title>
      <link>http://arxiv.org/abs/2512.04980v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mouad EL Bouchattaoui&lt;/p&gt;&lt;p&gt;This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.   The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04980v1</guid>
      <pubDate>Thu, 04 Dec 2025 16:51:49 +0000</pubDate>
    </item>
    <item>
      <title>Towards a unified framework for guided diffusion models</title>
      <link>http://arxiv.org/abs/2512.04985v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuchen Jiao, Yuxin Chen, Gen Li&lt;/p&gt;&lt;p&gt;Guided or controlled data generation with diffusion models\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04985v1</guid>
      <pubDate>Thu, 04 Dec 2025 16:55:20 +0000</pubDate>
    </item>
    <item>
      <title>Strategic Self-Improvement for Competitive Agents in AI Labour Markets</title>
      <link>http://arxiv.org/abs/2512.04988v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Christopher Chiu, Simpson Zhang, Mihaela van der Schaar&lt;/p&gt;&lt;p&gt;As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \textbf{metacognition} (accurate self-assessment of skills), \textbf{competitive awareness} (modeling rivals and market dynamics), and \textbf{lon&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04988v1</guid>
      <pubDate>Thu, 04 Dec 2025 16:57:28 +0000</pubDate>
    </item>
    <item>
      <title>Evolutionary Architecture Search through Grammar-Based Sequence Alignment</title>
      <link>http://arxiv.org/abs/2512.04992v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Adri Gómez Martín, Felix Möller, Steven McDonagh, Monica Abella, Manuel Desco, Elliot J. Crowley, Aaron Klein, Linus Ericsson&lt;/p&gt;&lt;p&gt;Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04992v1</guid>
      <pubDate>Thu, 04 Dec 2025 16:57:49 +0000</pubDate>
    </item>
    <item>
      <title>Reflection Removal through Efficient Adaptation of Diffusion Transformers</title>
      <link>http://arxiv.org/abs/2512.05000v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Daniyar Zakarin, Thiemo Wandel, Anton Obukhov, Dengxin Dai&lt;/p&gt;&lt;p&gt;We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05000v1</guid>
      <pubDate>Thu, 04 Dec 2025 17:12:39 +0000</pubDate>
    </item>
    <item>
      <title>Detecting Perspective Shifts in Multi-agent Systems</title>
      <link>http://arxiv.org/abs/2512.05013v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Eric Bridgeford, Hayden Helm&lt;/p&gt;&lt;p&gt;Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05013v1</guid>
      <pubDate>Thu, 04 Dec 2025 17:24:56 +0000</pubDate>
    </item>
    <item>
      <title>Model-Free Assessment of Simulator Fidelity via Quantile Curves</title>
      <link>http://arxiv.org/abs/2512.05024v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Garud Iyengar, Yu-Shiou Willy Lin, Kaizheng Wang&lt;/p&gt;&lt;p&gt;Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05024v1</guid>
      <pubDate>Thu, 04 Dec 2025 17:39:51 +0000</pubDate>
    </item>
    <item>
      <title>Arbitrage: Efficient Reasoning via Advantage-Aware Speculation</title>
      <link>http://arxiv.org/abs/2512.05033v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Monishwaran Maheswaran, Rishabh Tiwari, Yuezhou Hu, Kerem Dilmen, Coleman Hooper, Haocheng Xi, Nicholas Lee, Mehrdad Farajtabar&lt;/p&gt;&lt;p&gt;Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding stru&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05033v1</guid>
      <pubDate>Thu, 04 Dec 2025 17:50:53 +0000</pubDate>
    </item>
    <item>
      <title>QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory</title>
      <link>http://arxiv.org/abs/2512.05049v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yu-Chao Hsu, Jiun-Cheng Jiang, Chun-Hua Lin, Kuo-Chung Peng, Nan-Yow Chen, Samuel Yen-Chi Chen, En-Jui Kuo, Hsi-Sheng Goan&lt;/p&gt;&lt;p&gt;Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;br/&gt;- 任务：降尺度/预报/临近预测等应用场景。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05049v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:03:23 +0000</pubDate>
    </item>
    <item>
      <title>Meta-Learning for Quantum Optimization via Quantum Sequence Model</title>
      <link>http://arxiv.org/abs/2512.05058v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yu-Cheng Lin, Yu-Chao Hsu, Samuel Yen-Chi Chen&lt;/p&gt;&lt;p&gt;The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05058v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:13:45 +0000</pubDate>
    </item>
    <item>
      <title>Multi-LLM Collaboration for Medication Recommendation</title>
      <link>http://arxiv.org/abs/2512.05066v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Huascar Sanchez, Briland Hitaj, Jules Bergmann, Linda Briesemeister&lt;/p&gt;&lt;p&gt;As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05066v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:25:15 +0000</pubDate>
    </item>
    <item>
      <title>David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?</title>
      <link>http://arxiv.org/abs/2512.05073v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Animesh Basak Chowdhury, Sukanta Bhattacharjee, Chandan Karfa&lt;/p&gt;&lt;p&gt;Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents,&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05073v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:37:29 +0000</pubDate>
    </item>
    <item>
      <title>Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction</title>
      <link>http://arxiv.org/abs/2512.05092v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vincent Pauline, Tobias Höppe, Kirill Neklyudov, Alexander Tong, Stefan Bauer, Andrea Dittadi&lt;/p&gt;&lt;p&gt;Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05092v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:55:36 +0000</pubDate>
    </item>
    <item>
      <title>SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards</title>
      <link>http://arxiv.org/abs/2512.05098v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuan Gao, Jin Song&lt;/p&gt;&lt;p&gt;In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05098v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:58:18 +0000</pubDate>
    </item>
    <item>
      <title>Structured Document Translation via Format Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2512.05100v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haiyue Song, Johannes Eschbach-Dymanus, Hour Kaing, Sumire Honda, Hideki Tanaka, Bianka Buschbeck, Masao Utiyama&lt;/p&gt;&lt;p&gt;Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a f&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05100v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:58:30 +0000</pubDate>
    </item>
    <item>
      <title>TV2TV: A Unified Framework for Interleaved Language and Video Generation</title>
      <link>http://arxiv.org/abs/2512.05103v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen&lt;/p&gt;&lt;p&gt;Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05103v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:59:09 +0000</pubDate>
    </item>
    <item>
      <title>Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2512.05105v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Purbesh Mitra, Sennur Ulukus&lt;/p&gt;&lt;p&gt;Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05105v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:59:18 +0000</pubDate>
    </item>
    <item>
      <title>ShadowDraw: From Any Object to Shadow-Drawing Compositional Art</title>
      <link>http://arxiv.org/abs/2512.05110v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Rundong Luo, Noah Snavely, Wei-Chiu Ma&lt;/p&gt;&lt;p&gt;We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05110v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:59:51 +0000</pubDate>
    </item>
    <item>
      <title>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</title>
      <link>http://arxiv.org/abs/2512.05112v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye&lt;/p&gt;&lt;p&gt;Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05112v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:59:53 +0000</pubDate>
    </item>
    <item>
      <title>The Universal Weight Subspace Hypothesis</title>
      <link>http://arxiv.org/abs/2512.05117v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille&lt;/p&gt;&lt;p&gt;We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.05117v1</guid>
      <pubDate>Thu, 04 Dec 2025 18:59:58 +0000</pubDate>
    </item>
  </channel>
</rss>
