<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 11 Feb 2026 04:30:25 +0000</lastBuildDate>
    <item>
      <title>ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference</title>
      <link>http://arxiv.org/abs/2602.10004v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Junda Wang, Zhichao Yang, Dongxu Zhang, Sanjit Singh Batra, Robert E. Tillman&lt;/p&gt;&lt;p&gt;Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated &lt;stop&gt; signals, and (iii) &lt;stop&gt;-aware reinforceme&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10004v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:27:26 +0000</pubDate>
    </item>
    <item>
      <title>Discovering High Level Patterns from Simulation Traces</title>
      <link>http://arxiv.org/abs/2602.10009v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sean Memery, Kartic Subr&lt;/p&gt;&lt;p&gt;Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10009v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:31:39 +0000</pubDate>
    </item>
    <item>
      <title>A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula</title>
      <link>http://arxiv.org/abs/2602.10014v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Chenruo Liu, Yijun Dong, Yiqiu Shen, Qi Lei&lt;/p&gt;&lt;p&gt;Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10014v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:36:41 +0000</pubDate>
    </item>
    <item>
      <title>Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design</title>
      <link>http://arxiv.org/abs/2602.10016v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Bojian Hou, Xiaolong Liu, Xiaoyi Liu, Jiaqi Xu, Yasmine Badr, Mengyue Hang, Sudhanshu Chanpuriya, Junqing Zhou&lt;/p&gt;&lt;p&gt;Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10016v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:37:55 +0000</pubDate>
    </item>
    <item>
      <title>Online Selective Conformal Prediction with Asymmetric Rules: A Permutation Test Approach</title>
      <link>http://arxiv.org/abs/2602.10018v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mingyi Zheng, Ying Jin&lt;/p&gt;&lt;p&gt;Selective conformal prediction aims to construct prediction sets with valid coverage for a test unit conditional on it being selected by a data-driven mechanism. While existing methods in the offline setting handle any selection mechanism that is permutation invariant to the labeled data, their extension to the online setting -- where data arrives sequentially and later decisions depend on earlier ones -- is challenged by the fact that the selection mechanism is naturally asymmetric. As such, existing methods only address a limited collection of selection mechanisms&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10018v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:39:36 +0000</pubDate>
    </item>
    <item>
      <title>ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2602.10019v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qingnan Ren, Shiting Huang, Zhen Fang, Zehui Chen, Lin Chen, Lijun Li, Feng Zhao&lt;/p&gt;&lt;p&gt;Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10019v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:40:39 +0000</pubDate>
    </item>
    <item>
      <title>Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference</title>
      <link>http://arxiv.org/abs/2602.10021v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Wenxuan Xie, Yujia Wang, Xin Tan, Chaochao Lu, Xia Hu, Xuhong Wang&lt;/p&gt;&lt;p&gt;The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10021v1</guid>
      <pubDate>Tue, 10 Feb 2026 17:42:31 +0000</pubDate>
    </item>
    <item>
      <title>Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection</title>
      <link>http://arxiv.org/abs/2602.10042v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Changjiang Jiang, Xinkuan Sha, Fengchang Yu, Jingjing Liu, Jian Liu, Mingqi Fang, Chenfeng Zhang, Wei Lu&lt;/p&gt;&lt;p&gt;Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative det&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10042v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:10:08 +0000</pubDate>
    </item>
    <item>
      <title>Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2602.10044v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Akshay Mete, Shahid Aamir Sheikh, Tzu-Hsiang Lin, Dileep Kalathil, P. R. Kumar&lt;/p&gt;&lt;p&gt;Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10044v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:11:00 +0000</pubDate>
    </item>
    <item>
      <title>Conformal Prediction Sets for Instance Segmentation</title>
      <link>http://arxiv.org/abs/2602.10045v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kerri Lu, Dan M. Kluger, Stephen Bates, Sherrie Wang&lt;/p&gt;&lt;p&gt;Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has hig&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10045v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:15:06 +0000</pubDate>
    </item>
    <item>
      <title>Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization</title>
      <link>http://arxiv.org/abs/2602.10048v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xinchen Han, Hossam Afifi, Michel Marot, Xilu Wang, Lu Yin&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10048v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:15:58 +0000</pubDate>
    </item>
    <item>
      <title>WildCat: Near-Linear Attention in Theory and Practice</title>
      <link>http://arxiv.org/abs/2602.10056v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tobias Schröder, Lester Mackey&lt;/p&gt;&lt;p&gt;We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10056v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:22:32 +0000</pubDate>
    </item>
    <item>
      <title>Chain of Mindset: Reasoning with Adaptive Cognitive Modes</title>
      <link>http://arxiv.org/abs/2602.10063v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du&lt;/p&gt;&lt;p&gt;Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10063v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:31:47 +0000</pubDate>
    </item>
    <item>
      <title>Anagent For Enhancing Scientific Table &amp; Figure Analysis</title>
      <link>http://arxiv.org/abs/2602.10081v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang&lt;/p&gt;&lt;p&gt;In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \&amp; figure analysis&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10081v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:46:28 +0000</pubDate>
    </item>
    <item>
      <title>CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs</title>
      <link>http://arxiv.org/abs/2602.10085v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully&lt;/p&gt;&lt;p&gt;Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10085v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:51:39 +0000</pubDate>
    </item>
    <item>
      <title>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2602.10090v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He&lt;/p&gt;&lt;p&gt;Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10090v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:55:41 +0000</pubDate>
    </item>
    <item>
      <title>Causality in Video Diffusers is Separable from Denoising</title>
      <link>http://arxiv.org/abs/2602.10095v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xingjian Bai, Guande He, Zhengqi Li, Eli Shechtman, Xun Huang, Zongze Wu&lt;/p&gt;&lt;p&gt;Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10095v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:57:21 +0000</pubDate>
    </item>
    <item>
      <title>Step-resolved data attribution for looped transformers</title>
      <link>http://arxiv.org/abs/2602.10097v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Georgios Kaissis, David Mildenberger, Juan Felipe Gomez, Martin J. Menten, Eleni Triantafillou&lt;/p&gt;&lt;p&gt;We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$τ$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop ite&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10097v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:57:53 +0000</pubDate>
    </item>
    <item>
      <title>Olaf-World: Orienting Latent Actions for Video World Modeling</title>
      <link>http://arxiv.org/abs/2602.10104v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yuxin Jiang, Yuchao Gu, Ivor W. Tsang, Mike Zheng Shou&lt;/p&gt;&lt;p&gt;Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10104v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:58:41 +0000</pubDate>
    </item>
    <item>
      <title>Biases in the Blind Spot: Detecting What LLMs Fail to Mention</title>
      <link>http://arxiv.org/abs/2602.10117v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Iván Arcuschin, David Chanin, Adrià Garriga-Alonso, Oana-Maria Camburu&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2602.10117v1</guid>
      <pubDate>Tue, 10 Feb 2026 18:59:56 +0000</pubDate>
    </item>
  </channel>
</rss>
