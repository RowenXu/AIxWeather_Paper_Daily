<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 19 Dec 2025 03:24:50 +0000</lastBuildDate>
    <item>
      <title>PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy</title>
      <link>http://arxiv.org/abs/2512.16851v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque&lt;/p&gt;&lt;p&gt;The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP)&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16851v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:23:06 +0000</pubDate>
    </item>
    <item>
      <title>GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</title>
      <link>http://arxiv.org/abs/2512.16853v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Amita Kamath, Kai-Wei Chang, Ranjay Krishna, Luke Zettlemoyer, Yushi Hu, Marjan Ghazvininejad&lt;/p&gt;&lt;p&gt;Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16853v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:26:56 +0000</pubDate>
    </item>
    <item>
      <title>TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge</title>
      <link>http://arxiv.org/abs/2512.16855v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Khurram Khalil, Khaza Anuarul Hoque&lt;/p&gt;&lt;p&gt;Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16855v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:27:42 +0000</pubDate>
    </item>
    <item>
      <title>Distributional AGI Safety</title>
      <link>http://arxiv.org/abs/2512.16856v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Nenad Tomašev, Matija Franklin, Julian Jacobs, Sébastien Krier, Simon Osindero&lt;/p&gt;&lt;p&gt;AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safegua&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16856v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:29:50 +0000</pubDate>
    </item>
    <item>
      <title>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2512.16861v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett&lt;/p&gt;&lt;p&gt;Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16861v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:32:39 +0000</pubDate>
    </item>
    <item>
      <title>Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models</title>
      <link>http://arxiv.org/abs/2512.16866v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jiabin Xue&lt;/p&gt;&lt;p&gt;Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16866v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:37:28 +0000</pubDate>
    </item>
    <item>
      <title>Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</title>
      <link>http://arxiv.org/abs/2512.16871v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hesham G. Moussa, Aroosa Hameed, Arashmid Akhavain&lt;/p&gt;&lt;p&gt;To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16871v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:40:58 +0000</pubDate>
    </item>
    <item>
      <title>On the Universal Representation Property of Spiking Neural Networks</title>
      <link>http://arxiv.org/abs/2512.16872v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shayan Hundrieser, Philipp Tuchel, Insung Kong, Johannes Schmidt-Hieber&lt;/p&gt;&lt;p&gt;Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16872v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:41:51 +0000</pubDate>
    </item>
    <item>
      <title>The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI</title>
      <link>http://arxiv.org/abs/2512.16873v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Otman A. Basir&lt;/p&gt;&lt;p&gt;Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16873v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:42:16 +0000</pubDate>
    </item>
    <item>
      <title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
      <link>http://arxiv.org/abs/2512.16874v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko&lt;/p&gt;&lt;p&gt;Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16874v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:42:19 +0000</pubDate>
    </item>
    <item>
      <title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
      <link>http://arxiv.org/abs/2512.16876v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko&lt;/p&gt;&lt;p&gt;The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16876v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:44:13 +0000</pubDate>
    </item>
    <item>
      <title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
      <link>http://arxiv.org/abs/2512.16891v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu&lt;/p&gt;&lt;p&gt;Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constra&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16891v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:52:18 +0000</pubDate>
    </item>
    <item>
      <title>Impacts of Racial Bias in Historical Training Data for News AI</title>
      <link>http://arxiv.org/abs/2512.16901v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza&lt;/p&gt;&lt;p&gt;AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16901v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:56:11 +0000</pubDate>
    </item>
    <item>
      <title>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</title>
      <link>http://arxiv.org/abs/2512.16907v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian&lt;/p&gt;&lt;p&gt;Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16907v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:01 +0000</pubDate>
    </item>
    <item>
      <title>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</title>
      <link>http://arxiv.org/abs/2512.16911v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine&lt;/p&gt;&lt;p&gt;Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pr&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16911v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:17 +0000</pubDate>
    </item>
    <item>
      <title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
      <link>http://arxiv.org/abs/2512.16912v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin&lt;/p&gt;&lt;p&gt;This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploit&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16912v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:27 +0000</pubDate>
    </item>
    <item>
      <title>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2512.16917v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille&lt;/p&gt;&lt;p&gt;Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminat&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16917v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:54 +0000</pubDate>
    </item>
    <item>
      <title>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
      <link>http://arxiv.org/abs/2512.16921v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu&lt;/p&gt;&lt;p&gt;Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16921v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:57 +0000</pubDate>
    </item>
    <item>
      <title>DVGT: Driving Visual Geometry Transformer</title>
      <link>http://arxiv.org/abs/2512.16919v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang&lt;/p&gt;&lt;p&gt;Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16919v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:57 +0000</pubDate>
    </item>
    <item>
      <title>EasyV2V: A High-quality Instruction-based Video Editing Framework</title>
      <link>http://arxiv.org/abs/2512.16920v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei&lt;/p&gt;&lt;p&gt;While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.16920v1</guid>
      <pubDate>Thu, 18 Dec 2025 18:59:57 +0000</pubDate>
    </item>
  </channel>
</rss>
