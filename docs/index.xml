<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>arXiv · 气象 × AI 精选论文</title>
    <link>https://example.github.io/arxiv-meteo-ai-rss/</link>
    <description>每日10:00自动更新 · 气象与AI交叉最新论文与要点</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 04 Dec 2025 03:20:29 +0000</lastBuildDate>
    <item>
      <title>Hierarchical Vision Language Action Model Using Success and Failure Demonstrations</title>
      <link>http://arxiv.org/abs/2512.03913v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jeongeun Park, Jihwan Yoon, Byungwoo Jeon, Juhan Park, Jinwoo Shin, Namhoon Cho, Kyungjae Lee, Sangdoo Yun&lt;/p&gt;&lt;p&gt;Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03913v1</guid>
      <pubDate>Wed, 03 Dec 2025 15:58:38 +0000</pubDate>
    </item>
    <item>
      <title>A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models</title>
      <link>http://arxiv.org/abs/2512.03915v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; X. Y. Han, Yuan Zhong&lt;/p&gt;&lt;p&gt;In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03915v1</guid>
      <pubDate>Wed, 03 Dec 2025 16:00:02 +0000</pubDate>
    </item>
    <item>
      <title>Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties</title>
      <link>http://arxiv.org/abs/2512.03931v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Vineel Tummala, Daniela Inclezan&lt;/p&gt;&lt;p&gt;This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03931v1</guid>
      <pubDate>Wed, 03 Dec 2025 16:29:09 +0000</pubDate>
    </item>
    <item>
      <title>Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol</title>
      <link>http://arxiv.org/abs/2512.03955v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Niklas Jobs, Luis Miguel Vieira da Silva, Jayanth Somashekaraiah, Maximilian Weigand, David Kube, Felix Gehlhoff&lt;/p&gt;&lt;p&gt;Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03955v1</guid>
      <pubDate>Wed, 03 Dec 2025 16:49:14 +0000</pubDate>
    </item>
    <item>
      <title>Sponsored Questions and How to Auction Them</title>
      <link>http://arxiv.org/abs/2512.03975v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kshipra Bhawalkar, Alexandros Psomas, Di Wang&lt;/p&gt;&lt;p&gt;Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03975v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:06:27 +0000</pubDate>
    </item>
    <item>
      <title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
      <link>http://arxiv.org/abs/2512.03979v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jin-Ting He, Fu-Jen Tsai, Yan-Tsung Peng, Min-Hung Chen, Chia-Wen Lin, Yen-Yu Lin&lt;/p&gt;&lt;p&gt;Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03979v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:10:44 +0000</pubDate>
    </item>
    <item>
      <title>DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</title>
      <link>http://arxiv.org/abs/2512.03992v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zexin Lin, Hawen Wan, Yebin Zhong, Xiaoqiang&lt;/p&gt;&lt;p&gt;Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03992v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:22:29 +0000</pubDate>
    </item>
    <item>
      <title>Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation</title>
      <link>http://arxiv.org/abs/2512.03996v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hang Xu, Linjiang Huang, Feng Zhao&lt;/p&gt;&lt;p&gt;Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enh&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.03996v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:27:53 +0000</pubDate>
    </item>
    <item>
      <title>Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</title>
      <link>http://arxiv.org/abs/2512.04000v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Jialuo Li, Bin Li, Jiahao Li, Yan Lu&lt;/p&gt;&lt;p&gt;The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04000v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:36:06 +0000</pubDate>
    </item>
    <item>
      <title>Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics</title>
      <link>http://arxiv.org/abs/2512.04006v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Connall Garrod, Jonathan P. Keating, Christos Thrampoulidis&lt;/p&gt;&lt;p&gt;Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extens&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04006v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:45:09 +0000</pubDate>
    </item>
    <item>
      <title>On the Temporality for Sketch Representation Learning</title>
      <link>http://arxiv.org/abs/2512.04007v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Marcelo Isaias de Moraes Junior, Moacir Antonelli Ponti&lt;/p&gt;&lt;p&gt;Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04007v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:46:05 +0000</pubDate>
    </item>
    <item>
      <title>TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees</title>
      <link>http://arxiv.org/abs/2512.04016v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Davut Emre Tasar, Ceren Ocal Tasar&lt;/p&gt;&lt;p&gt;Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04016v1</guid>
      <pubDate>Wed, 03 Dec 2025 17:53:38 +0000</pubDate>
    </item>
    <item>
      <title>PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</title>
      <link>http://arxiv.org/abs/2512.04025v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Xiaolong Li, Youping Gu, Xi Lin, Weijie Wang, Bohan Zhuang&lt;/p&gt;&lt;p&gt;Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04025v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:02:11 +0000</pubDate>
    </item>
    <item>
      <title>Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study</title>
      <link>http://arxiv.org/abs/2512.04031v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yixuan Li, Yuhao Lu, Yang Liu, Liang Li, R. Ruffini, Di Li, Rong-Gen Cai, Xiaoyan Zhu&lt;/p&gt;&lt;p&gt;This work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with inc&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04031v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:13:01 +0000</pubDate>
    </item>
    <item>
      <title>Jina-VLM: Small Multilingual Vision Language Model</title>
      <link>http://arxiv.org/abs/2512.04032v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Andreas Koukounas, Georgios Mastrapas, Florian Hönicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao&lt;/p&gt;&lt;p&gt;We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04032v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:13:41 +0000</pubDate>
    </item>
    <item>
      <title>Fast &amp; Efficient Normalizing Flows and Applications of Image Generative Models</title>
      <link>http://arxiv.org/abs/2512.04039v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Sandeep Nagar&lt;/p&gt;&lt;p&gt;This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk con&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04039v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:29:03 +0000</pubDate>
    </item>
    <item>
      <title>MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking</title>
      <link>http://arxiv.org/abs/2512.04044v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Yizhou Zhao, Zhiwei Steven Wu, Adam Block&lt;/p&gt;&lt;p&gt;Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable &lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 数据：包含再分析/卫星/观测等来源。&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04044v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:32:19 +0000</pubDate>
    </item>
    <item>
      <title>Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs</title>
      <link>http://arxiv.org/abs/2512.04047v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Nadav Kunievsky&lt;/p&gt;&lt;p&gt;In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constrain&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04047v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:33:26 +0000</pubDate>
    </item>
    <item>
      <title>Fare Comparison App of Uber, Ola and Rapido</title>
      <link>http://arxiv.org/abs/2512.04065v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Ashlesha Gopinath Sawant, Sahil S. Jadhav, Vidhan R. Jain, Shriraj S. Jagtap, Prachi Jadhav, Soham Jadhav, Ichha Raina&lt;/p&gt;&lt;p&gt;In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 摘要未提供更多细节，建议阅读原文。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04065v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:48:33 +0000</pubDate>
    </item>
    <item>
      <title>SkillFactory: Self-Distillation For Learning Cognitive Behaviors</title>
      <link>http://arxiv.org/abs/2512.04072v1</link>
      <description>&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zayne Sprague, Jack Lu, Manya Wadhwa, Sedrick Keh, Mengye Ren, Greg Durrett&lt;/p&gt;&lt;p&gt;Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL&lt;br/&gt;&lt;br/&gt;要点：&lt;br/&gt;- 方法：采用机器学习/深度学习模型。&lt;/p&gt;</description>
      <guid isPermaLink="false">2512.04072v1</guid>
      <pubDate>Wed, 03 Dec 2025 18:54:53 +0000</pubDate>
    </item>
  </channel>
</rss>
